* Cell-free DNA Fragmentomics                                       :biopipe:
:PROPERTIES:
:header-args: :tangle no :mkdirp yes :tangle-mode (identity #o555)
:ID:       f0fbade8-2251-4aec-958f-ac1e1edd6c18
:END:
** Repository setup and administration
:PROPERTIES:
:ID:       2b64f328-8636-4068-8d09-9d698cc26822
:END:
- Snakemake run with conda
  #+begin_src bash :tangle ./tools/smk.sh
#!/usr/bin/env bash

#######################################
###   Snakemake Conda Run Wrapper   ###
#######################################

#bash.usage

# Variables
env="${1}"
config="${2}"
snkfile="${3}"

# Necessary to run conda snakemake command in shell script
eval "$(command conda 'shell.bash' 'hook' 2> /dev/null)"

conda activate $env

run_dry(){
    snakemake \
        --configfile $config \
        --cores 1 \
        --dry-run \
        --printshellcmds \
        --snakefile $snkfile
}

run(){
    snakemake \
        --configfile $config \
        --cores $nproc \
        --printshellcmds \
        --snakefile $snkfile
}

if [ "$4" == "run" ]; then
    run $env $config $snkfile
else
    run_dry $env $config $snkfile
fi
#+end_src
- Tangle org file
  #+begin_src bash :tangle ./tools/org_tangle.sh :tangle-mode (identity #o555)
#!/usr/bin/env bash

#################################################################
###   Tangle An Org-Mode File Through Non-Interactive Emacs   ###
#################################################################

org_file="${1}"


/usr/bin/emacs --batch -l org --eval "(progn (find-file \"$org_file\") (org-babel-tangle))"

#+end_src
- Conda build and update
  #+begin_src bash :tangle ./tools/conda_yaml_build_and_update.sh
#!/usr/bin/env bash

print_usage(){
    cat <<- EOF

usage: conda_yaml_build_and_update.sh <CONDA ENV YAML FILE>

Conda environment builder and updater
Assumes mamba is present in your base conda environment.

example: conda_yaml_build_and_update.sh test_env.yaml

EOF
}

# Path to the YAML file
yaml_path=$1

# Necessary to run conda in shell script
eval "$(command conda 'shell.bash' 'hook' 2> /dev/null)"

conda activate base

build_or_update(){
    # Name of the Conda environment
    env_name=$(head $yaml_path -n 1 | sed 's/^.*:.//g')
    # Check if the environment already exists
    if conda env list | grep -q "^$env_name\s"; then
        # Update the existing environment
        mamba env update -n "$env_name" -f "$yaml_path"
    else
        # Create a new environment
        mamba env create -n "$env_name" -f "$yaml_path"
    fi
}

conda deactivate

if [ $# -ne 1 ]; then print_usage && exit 1; fi

build_or_update
#+end_src

*** Bash source
:PROPERTIES:
:ID:       1d8ba95c-8206-4b6b-9086-e9503193ec86
:END:
#+begin_src bash :tangle ./config/bash_src
data_dir="/mnt/ris/szymanski/Active/test"
#+end_src
*** Setup inputs for testing
:PROPERTIES:
:ID:       439cfd95-f124-4597-95b3-085a912bc5b9
:END:
- blacklist
  #+begin_src bash
source config/bash_src
wget --directory-prefix="${data_dir}/inputs" \
     --no-clobber \
     wget https://github.com/Boyle-Lab/Blacklist/raw/master/lists/hg38-blacklist.v2.bed.gz
#+end_src
- hg38
  #+begin_src bash
source config/bash_src
wget --directory-prefix="${data_dir}/inputs" \
     --no-clobber \
     ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz
#+end_src
- cfDNA WGS fastqs
  #+begin_src bash
source config/bash_src

mntpt=/mnt/ris/aadel/Active

fq_size=1000000

zcat ${mntpt}/mpnst/inputs/seq/MPNST/19_2_082_R1.fastq.gz | head -n $fq_size > ${data_dir}/inputs/19_2_082_R1.fastq.gz
zcat ${mntpt}/mpnst/inputs/seq/MPNST/19_2_082_R2.fastq.gz | head -n $fq_size > ${data_dir}/inputs/19_2_082_R2.fastq.gz

zcat ${mntpt}/mpnst/inputs/seq/MPNST/25_2_072_R1.fastq.gz | head -n $fq_size > ${data_dir}/inputs/25_2_072_R1.fastq
zcat ${mntpt}/mpnst/inputs/seq/MPNST/25_2_072_R2.fastq.gz | head -n $fq_size > ${data_dir}/inputs/25_2_072_R2.fastq

zcat ${mntpt}/mpnst/inputs/seq/PN/37_JS0050CD112717_R1.fastq.gz | head -n $fq_size > ${data_dir}/inputs/JS0050CD112717_R1.fastq
zcat ${mntpt}/mpnst/inputs/seq/PN/37_JS0050CD112717_R2.fastq.gz | head -n $fq_size > ${data_dir}/inputs/JS0050CD112717_R2.fastq

zcat ${mntpt}/mpnst/inputs/seq/KO_10_23_17_R1.fastq.gz | head -n $fq_size > ${data_dir}/inputs/KO_10_23_17_R1.fastq.gz
zcat ${mntpt}/mpnst/inputs/seq/KO_10_23_17_R2.fastq.gz | head -n $fq_size > ${data_dir}/inputs/KO_10_23_17_R2.fastq.gz

zcat ${mntpt}/mpnst/inputs/seq/TS_36M_R1.fastq.gz | head -n $fq_size > ${data_dir}/inputs/TS_36M_R1.fastq.gz
zcat ${mntpt}/mpnst/inputs/seq/TS_36M_R2.fastq.gz | head -n $fq_size > ${data_dir}/inputs/TS_36M_R2.fastq.gz

zcat ${mntpt}/mpnst/inputs/seq/KS_30F_R1.fastq.gz | head -n $fq_size > ${data_dir}/inputs/KS_30F_R1.fastq.gz
zcat ${mntpt}/mpnst/inputs/seq/KS_30F_R2.fastq.gz | head -n $fq_size > ${data_dir}/inputs/KS_30F_R2.fastq.gz

for file in ${data_dir}/inputs/*.fastq; do gzip -f $file; done
#+end_src
- frag_ligs.tsv [[file:/mnt/ris/szymanski/Active/test/inputs/frag_libs.tsv]]
  | library | r1_basename             | cohort    |
  |---------+-------------------------+-----------|
  | lib001  | 19_2_082_R1.fastq.gz    | mpnst     |
  | lib002  | 25_2_072_R1.fastq       | mpnst     |
  | lib003  | JS0050CD112717_R1.fastq | plexiform |
  | lib004  | KO_10_23_17_R1.fastq.gz | plexiform |
  | lib005  | TS_36M_R1.fastq.gz      | healthy   |
  | lib006  | KS_30F_R1.fastq.gz      | healthy   |
** [[file:workflow/frag_reads.smk][Basic read processing]] :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflows/read_processing.smk
:ID:       5121ade9-a29a-451f-a14d-227b967d8f0a
:END:
*** Preamble
:PROPERTIES:
:ID:       a6a9f575-a201-4661-8c3d-fe1cfd076707
:END:
#+begin_src snakemake
#########1#########2#########3#########4#########5#########6#########7#########8
###                                                                          ###
###                    Basic Read Processing of WGS cfDNA                    ###
###                                                                          ###
#########1#########2#########3#########4#########5#########6#########7#########8
#+end_src
*** Make BWA index
#+begin_src snakemake
rule cfdna_wgs_index:
    output:
        f"{ref_dir}/{{build}}_bwa/{{build}}.fa.sa",
        f"{ref_dir}/{{build}}_bwa/{{build}}.fa",
    params:
        ref_dir = f"{ref_dir}",
        ftp = lambda wildcards: genome_build_map[wildcards.build]['ftp'],
        out_prefix = f"{ref_dir}/{{build}}_bwa/{{build}}.fa",
    shell:
        """
        base=$(basename "{params.ftp}")
        wget -N -P {params.ref_dir} {params.ftp}
        gunzip -c {params.ref_dir}/$base > {params.out_prefix}
        bwa index {params.out_prefix}
        """
#+end_src
- Shell script
  #+begin_src bash :tangle ./scripts/frag_index.sh
#!/usr/bin/env bash
in_fasta="${1}"
out_prefix="${2}"

bwa index -p $out_prefix $in_fasta
#+end_src
*** Fastq adapter and quality trimming with fastp

Adapter-trim and QC reads with fastp

#+begin_src snakemake
rule cfdna_wgs_fastp:
    input:
        read1 = f"{cfdna_wgs_dir}/fastqs/{{library}}_raw_R1.fastq.gz",
        read2 = f"{cfdna_wgs_dir}/fastqs/{{library}}_raw_R2.fastq.gz",
    log:
        html = f"{log_dir}/{{library}}_cfdna_wgs_fastp.html",
    output:
        read1 = f"{cfdna_wgs_dir}/fastqs/{{library}}_proc_R1.fastq.gz",
        read2 = f"{cfdna_wgs_dir}/fastqs/{{library}}_proc_R2.fastq.gz",
        failed = f"{cfdna_wgs_dir}/fastqs/{{library}}_failed_fastp.fastq.gz",
        unpaired1 = f"{cfdna_wgs_dir}/fastqs/{{library}}_unpaired_R1.fastq.gz",
        unpaired2 = f"{cfdna_wgs_dir}/fastqs/{{library}}_unpaired_R2.fastq.gz",
        json = f"{qc_dir}/{{library}}_cfdna_wgs_fastp.json",
        cmd = f"{qc_dir}/{{library}}_cfdna_wgs_fastp.log",
    params:
        script = f"{cfdna_script_dir}/fastp.sh",
        threads = threads,
    resources:
        mem_mb = 500,
    shell:
        """
        {params.script} \
        {input.read1} \
        {input.read2} \
        {log.html} \
        {output.json} \
        {output.read1} \
        {output.read2} \
        {output.failed} \
        {output.unpaired1} \
        {output.unpaired2} \
        {params.threads} &> {output.cmd}
        """
#+end_src
- [[file:scripts/fastp.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/fastp.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables

input_read1="${1}"
input_read2="${2}"
log_html="${3}"
log_json="${4}"
output_read1="${5}"
output_read2="${6}"
output_failed="${7}"
output_unpaired1="${8}"
output_unpaired2="${9}"
params_threads="${10}"

# Functions
main(){
    fastp_wrap $output_failed \
               $input_read1 \
               $input_read2 \
               $log_html \
               $log_json \
               $output_read1 \
               $output_read2 \
               $output_unpaired1 \
               $output_unpaired2 \
               $params_threads
}

fastp_wrap(){
    fastp --detect_adapter_for_pe \
          --failed_out $output_failed \
          --in1 $input_read1 \
          --in2 $input_read2 \
          --html $log_html \
          --json $log_json \
          --out1 $output_read1 \
          --out2 $output_read2 \
          --unpaired1 $output_unpaired1 \
          --unpaired2 $output_unpaired2 \
          --thread $params_threads
    }

# Run
main "$@"
#+end_src


*** Read and alignment processing
:PROPERTIES:
:ID:       821ba448-d96e-4da7-9c48-499250c976a5
:END:
**** Align reads with BWA                                          :smk_rule:
:PROPERTIES:
:ID:       224c54ff-b8bf-45ba-882b-c368d98d19f3
:END:
- [[./workflow/reads.smk::rule cfdna_wgs_align][Snakemake]]
  #+begin_src snakemake
# Align reads with BWA
rule frag_align:
    benchmark: f"{bench_dir}/{{library}}_{{build}}_frag_align.benchmark.txt",
    input:
        ref = f"{ref_dir}/{{build}}_bwa/{{build}}.fa.sa",
        read1 = f"{cfdna_wgs_dir}/fastqs/{{library}}_proc_R1.fastq.gz",
        read2 = f"{cfdna_wgs_dir}/fastqs/{{library}}_proc_R2.fastq.gz",
    log: f"{log_dir}/{{library}}_{{build}}frag_align.log",
    output:
        sort = f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_raw.bam",
        index = f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_raw.bam.bai",
    params:
        ref = f"{ref_dir}/{{build}}_bwa/{{build}}.fa",
        script = f"{cfdna_script_dir}/align.sh",
        threads = 4,
    resources:
        mem_mb = 500,
    shell:
        """
        {params.script} \
        {params.ref} \
        {input.read1} \
        {input.read2} \
        {params.threads} \
        {output.sort} &> {log}
        """
#+end_src
- [[file:scripts/align.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/align.sh
#!/usr/bin/env bash
input_ref=$1
input_r1=$2
input_r2=$3
threads=$4
output_sort=$5

bwa mem -M -t $threads \
    $input_ref \
    $input_r1 \
    $input_r2 |
    samtools view -@ $threads --bam - -o - |
    samtools sort -@ $threads - -o $output_sort && samtools index -@ threads $output_sort
#+end_src
**** Remove PCR duplicates                                         :smk_rule:
:PROPERTIES:
:ID:       334c3208-f64d-4ae4-bd3b-6bb3f8debef6
:END:
- [[./workflow/reads.smk::rule cfdna_wgs_dedup][Snakemake]]
  #+begin_src snakemake
# Remove PCR duplicates from aligned reads
rule frag_dedup:
    benchmark: f"{bench_dir}/{{library}}_{{build}}_frag_dedup.benchmark.txt",
    input: f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_raw.bam",
    log: f"{log_dir}/{{library}}_{{build}}_frag_dedup.log",
    output: f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_dedup.bam",
    params:
        script = f"{cfdna_script_dir}/dedup.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        {input} \
        {output} \
        {params.threads} &> {log}
        """
#+end_src
- [[file:scripts/dedup.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/dedup.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables
raw_bam="${1}"
dedup_bam="${2}"
threads="${3}"

samtools sort -@ $threads -n -o - $raw_bam |
    samtools fixmate -m - - |
    samtools sort -@ $threads -o - - |
    samtools markdup -@ $threads -r - $dedup_bam
samtools index $dedup_bam
#+end_src
**** Filter de-duplicated alignments                               :smk_rule:
:PROPERTIES:
:ID:       ed7f2412-8ad9-4ca1-a387-bd7b3707da69
:END:

Filter de-duplicated alignments.
Remove unmapped, not primary, and duplicate reads.

#+begin_src snakemake

rule frag_filter_alignment:
    benchmark: f"{bench_dir}/{{library}}_{{build}}_frag_filter_alignment.benchmark.txt",
    input: f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_dedup.bam",
    log: f"{log_dir}/{{library}}_{{build}}_frag_filter_alignment.log",
    output: f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_filt.bam",
    params:
        script = f"{cfdna_script_dir}/filter_alignment.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        {input} \
        {params.threads} \
        {output} &> {log}
        """
#+end_src
- [[file:scripts/filter_alignment.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/filter_alignment.sh
#!/usr/bin/env bash

input=$1
threads=$2
output=$3

# Filter to reads that are
#  - Excluding any unmapped, not primary alignment, or duplicates
#  - Only MAPQ > 20
# DO NOT restrict to "proper pairs"- this clips long cfDNA fragments!

samtools view -@ $threads -b -F 1284 -h -q 20 -o $output $input

samtools index ${output}
#+end_src
*** Read and alignment QC
:PROPERTIES:
:ID:       57a875a7-f8ba-4e11-b2d7-a58bfe8a15e0
:END:
**** FastQC                                                        :smk_rule:
:PROPERTIES:
:ID:       12e81197-042f-432e-a83d-fffb5518e908
:END:
- [[./workflow/reads.smk::rule cfdna_wgs_fastqc][Snakemake]]
  #+begin_src snakemake
# Get read quality by FASTQC
rule frag_fastqc:
    benchmark: f"{bench_dir}/{{library}}_{{processing}}_{{read}}_frag_fastqc.benchmark.txt",
    input: f"{cfdna_wgs_dir}/fastqs/{{library}}_{{processing}}_{{read}}.fastq.gz",
    log: f"{log_dir}/{{library}}_{{processing}}_{{read}}_frag_fastqc.log",
    output:
        f"{qc_dir}/{{library}}_{{processing}}_{{read}}_fastqc.html",
        f"{qc_dir}/{{library}}_{{processing}}_{{read}}_fastqc.zip",
    params:
        outdir = f"{qc_dir}",
        script = f"{cfdna_script_dir}/fastqc.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        {input} \
        {params.outdir} \
        {params.threads} &> {log}
        """
#+end_src
- [[file:scripts/fastqc.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/fastqc.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables
input="${1}"
outdir="${2}"
threads="${3}"

# Functions
fastqc  --outdir $outdir \
        --quiet \
        --threads $threads $input
#+end_src
**** Alignment QC                                                  :smk_rule:
:PROPERTIES:
:ID:       6f624cb4-4379-4ebb-8431-10ef3690843b
:END:
#+begin_src snakemake
# Get alignment QC using samtools
rule frag_alignment_qc:
    input: f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_{{processing}}.bam",
    log:
        flagstat = f"{log_dir}/{{library}}_{{build}}_{{processing}}_flagstat_frag_alignment_qc.log",
        samstat = f"{log_dir}/{{library}}_{{build}}_{{processing}}_samstats_frag_alignment_qc.log",
    output:
        flagstat = f"{qc_dir}/{{library}}_{{build}}_{{processing}}_flagstat.txt",
        samstat = f"{qc_dir}/{{library}}_{{build}}_{{processing}}_samstats.txt",
    params:
        script = f"{cfdna_script_dir}/alignment_qc.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        {input} \
        {log.flagstat} \
        {log.samstat} \
        {output.flagstat} \
        {output.samstat} \
        {params.threads}
        """
#+end_src
- [[file:scripts/alignment_qc.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/alignment_qc.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables
input="${1}"
log_flagstat="${2}"
log_samstat="${3}"
output_flagstat="${4}"
output_samstat="${5}"
threads="${6}"

# Functions
main(){
    flagstat $input $output_flagstat $log_flagstat $threads
    samstats $input $output_samstat $log_samstat $threads
}

flagstat(){
    local input="${1}"
    local output="${2}"
    local log="${3}"
    local threads="${4}"
    #
    samtools flagstat -@ $threads $input > $output 2>$log
}

samstats(){
    local input="${1}"
    local output="${2}"
    local log="${3}"
    local threads="${4}"
    #
    samtools stats -@ $threads $input > $output 2>$log
}

# Run
main "$@"
#+end_src
*** Sequencing depth metrics via Picard                            :smk_rule:
:PROPERTIES:
:ID:       0689d6cd-de86-4c12-a76b-52885a20ed8f
:END:
- [[./workflow/reads.smk::rule cfdna_wgs_picard_depth][Snakemake]]
  #+begin_src snakemake
# Sequencing depth metrics via Picard
rule frag_picard_depth:
    benchmark: f"{bench_dir}/{{library}}_{{build}}_frag_picard_depth.benchmark.txt",
    input:
        ref = f"{ref_dir}/{{build}}_bwa/{{build}}.fa",
        bam = f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_filt.bam",
    log: f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_frag_picard_depth.log",
    output: f"{qc_dir}/{{library}}_{{build}}_picard_depth.txt",
    params:
        script = f"{cfdna_script_dir}/picard_depth.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        {input.bam} \
        {input.ref} \
        {output}
        """
#+end_src
- [[file:scripts/picard_depth.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/picard_depth.sh
#!/usr/bin/env bash
input=$1
genome_fasta=$2
output=$3

picard CollectWgsMetrics \
       INPUT=$input \
       OUTPUT=$output \
       READ_LENGTH=150 \
       REFERENCE_SEQUENCE=$genome_fasta
#+end_src
*** deepTools fragment sizes                                       :smk_rule:
:PROPERTIES:
:ID:       6c4c06ac-f963-426a-95e9-2d4772374035
:END:
- [[./workflow/reads.smk::rule cfdna_wgs_bampefragsize][Snakemake]]
  #+begin_src snakemake
# Get fragment sizes using deepTools
rule frag_bampefragsize:
    conda: "deeptools",
    input:
        lambda wildcards: expand(f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_filt.bam",
                                 library = lib_map[wildcards.lib_set]['libs'],
                                 build = lib_map[wildcards.lib_set]['build']),
    log: f"{log_dir}/{{lib_set}}_bampefragsize.log",
    output:
        raw = f"{qc_dir}/deeptools_{{lib_set}}_lengths.txt",
        hist = f"{qc_dir}/deeptools_{{lib_set}}_lengths.png",
    params:
        blacklist = lambda wildcards: lib_map[wildcards.lib_set]['blacklist'],
        script = f"{cfdna_script_dir}/bampefragsize.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        "{input}" \
        {log} \
        {output.hist} \
        {output.raw} \
        {params.blacklist} \
        {params.threads}
        """
#+end_src
- [[file:scripts/bampefragsize.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/bampefragsize.sh
#!/usr/bin/env bash
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables

input="${1}"
log="${2}"
output_hist="${3}"
output_raw="${4}"
blacklist="${5}"
threads="${6}"


bamPEFragmentSize --bamfiles $input \
                  --numberOfProcessors $threads \
                  --blackListFileName $blacklist \
                  --histogram $output_hist \
                  --maxFragmentLength 1000 \
                  --outRawFragmentLengths $output_raw
#+end_src
*** deepTools plotCoverage                                         :smk_rule:
:PROPERTIES:
:ID:       fd5c9058-8476-48d8-8a45-0784dde9e401
:END:
- [[./workflow/reads.smk::rule cfdna_wgs_plotcoverage][Snakemake]]
  #+begin_src snakemake
# Make deepTools plotCoverage coverage maps for all filtered bams
rule frag_plotcoverage:
    conda: "deeptools",
    input:
        lambda wildcards: expand(f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_filt.bam",
                                 library = lib_map[wildcards.lib_set]['libs'],
                                 build = lib_map[wildcards.lib_set]['build']),
    log: f"{log_dir}/{{lib_set}}_frag_plotcoverage.log",
    output:
        raw = f"{qc_dir}/{{lib_set}}_frag_coverage.tsv",
        plot = f"{qc_dir}/{{lib_set}}_frag_coverage.pdf",
    params:
        blacklist = lambda wildcards: lib_map[wildcards.lib_set]['blacklist'],
        script = f"{cfdna_script_dir}/plotcoverage.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        "{input}" \
        {params.blacklist} \
        {params.threads} \
        {output.raw} \
        {output.plot} &> {log}
        """
#+end_src
- [[file:scripts/plotcoverage.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/plotcoverage.sh
#!/usr/bin/env bash
in_bam_string=$1
blacklist=$2
threads=$3
out_raw=$4
out_plot=$5

plotCoverage \
    --bamfiles $in_bam_string \
    --blackListFileName $blacklist \
    --extendReads \
    --numberOfProcessors $threads \
    --outRawCounts $out_raw \
    --plotFile $out_plot \
    --plotFileFormat pdf \
    --skipZeros
#+end_src
*** MultiQC                                                        :smk_rule:
:PROPERTIES:
:ID:       a3f3c091-2c6d-4509-9881-06ff186fb45d
:END:
#+begin_src snakemake
# Aggregate QC files using MultiQC
rule frag_multiqc:
    input:
        lambda wildcards: expand(f"{qc_dir}/{{library}}_cfdna_wgs_fastp.json",
                                 library = lib_map[wildcards.lib_set]['libs']),
        lambda wildcards: expand(f"{qc_dir}/{{library}}_{{processing}}_{{read}}_fastqc.zip",
                                 library = lib_map[wildcards.lib_set]['libs'],
                                 processing = ['raw','proc',],
                                 read = ['R1','R2']),
        lambda wildcards: expand(f"{qc_dir}/{{library}}_{{build}}_{{processing}}_samstats.txt",
                                 library = lib_map[wildcards.lib_set]['libs'],
                                 build = lib_map[wildcards.lib_set]['build'],
                                 processing = ['raw','dedup','filt']),
        lambda wildcards: expand(f"{qc_dir}/{{library}}_{{build}}_{{processing}}_flagstat.txt",
                                 library = lib_map[wildcards.lib_set]['libs'],
                                 build = lib_map[wildcards.lib_set]['build'],
                                 processing = ['raw','dedup','filt']),
        lambda wildcards: expand(f"{qc_dir}/{{library}}_{{build}}_picard_depth.txt",
                                 library = lib_map[wildcards.lib_set]['libs'],
                                 build = lib_map[wildcards.lib_set]['build']),
        f"{qc_dir}/deeptools_{{lib_set}}_lengths.txt",
        f"{qc_dir}/{{lib_set}}_frag_coverage.tsv",
    log: f"{log_dir}/{{lib_set}}_cfdna_wgs_multiqc.log",
    output:
        f"{qc_dir}/{{lib_set}}_cfdna_wgs_multiqc.html",
        f"{qc_dir}/{{lib_set}}_cfdna_wgs_multiqc_fastqc.txt",
        f"{qc_dir}/{{lib_set}}_cfdna_wgs_multiqc_data/multiqc_samtools_stats.txt",
        f"{qc_dir}/{{lib_set}}_cfdna_wgs_multiqc_data/multiqc_picard_wgsmetrics.txt",
        f"{qc_dir}/{{lib_set}}_cfdna_wgs_multiqc_data/multiqc_samtools_flagstat.txt",
    params:
        out_dir = f"{qc_dir}",
        out_name = "frag_multiqc",
        script = f"{cfdna_script_dir}/multiqc.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        "{input}" \
        {params.out_name} \
        {params.out_dir} &> {log}
        """
#+end_src
- [[file:scripts/multiqc.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/multiqc.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables

   input="${1}"
out_name="${2}"
 out_dir="${3}"

# Functions

multiqc $input \
        --force \
        --outdir $out_dir \
        --filename $out_name
#+end_src

*** Make aggregate QC table                                        :smk_rule:
:PROPERTIES:
:ID:       22dd84a2-de3e-4faf-8b54-d1408c76aa33
:END:

#+begin_src snakemake
# Make a tab-separated aggregate QC table
rule make_cfdna_wgs_qc_tsv:
    input:
        fq = f"{qc_dir}/{{lib_set}}_cfdna_wgs_multiqc_data/multiqc_fastqc.txt",
        mqsam = f"{qc_dir}/{{lib_set}}_cfdna_wgs_multiqc_data/multiqc_samtools_stats.txt",
        mqflag = f"{qc_dir}/{{lib_set}}_cfdna_multiqc_data/multiqc_samtools_flagstat.txt",
        picard = f"{qc_dir}/{{lib_set}}_multiqc_data/multiqc_picard_wgsmetrics.txt",
        deeptools_frag = f"{qc_dir}/{{lib_set}}_deeptools_cfdna_wgs_lengths.txt",
        deeptools_cov = f"{qc_dir}/{{lib_set}}_cfdna_wgs_coverage.tsv",
    log: f"{log_dir}/{{lib_set}}_cfdna_wgs_make_qc_tsv.log",
    output:
        readqc = f"{qc_dir}/{{lib_set}}_cfdna_wgs_read_qc.tsv",
        fraglen = f"{qc_dir}/{{lib_set}}_cfdna_wgs_len.tsv",
    params:
        script = f"{cfdna_script_dir}/make_qc_tsv.R",
    shell:
        """
        Rscript {params.script} \
        {input.fq} \
        {input.mqsam} \
        {input.mqflag} \
        {input.picard} \
        {input.deeptools_frag} \
        {input.deeptools_cov} \
        {output.readqc} \
        {output.fraglen} >& {log}
        """
#+end_src
- [[file:scripts/make_qc_tsv.R][Rscript]]
  #+begin_src R :tangle ./scripts/make_qc_tsv.R
#!/usr/bin/env Rscript
#
# Unit test variables
## mqc_dir="test/analysis/qc/frag_multiqc_data"
## fastqc_input = paste0(mqc_dir,"/multiqc_fastqc.txt")
## samstats_input = paste0(mqc_dir, "/multiqc_samtools_stats.txt")
## flagstats_input = paste0(mqc_dir, "/multiqc_samtools_flagstat.txt")
## picard_input = paste0(mqc_dir, "/multiqc_picard_wgsmetrics.txt")
## deeptools_frag_input = "test/analysis/qc/deeptools_frag_lengths.txt"
## deeptools_cov_input = "test/analysis/qc/frag_coverage.tsv"

args = commandArgs(trailingOnly = TRUE)
fastqc_input = args[1]
samstats_input = args[2]
flagstats_input = args[3]
picard_input = args[4]
deeptools_frag_input = args[5]
deeptools_cov_input = args[6]
readqc_out_tbl = args[7]
frag_len_out_tbl = args[8]

library(tidyverse)

process_multiqc_fastqc = function(multiqc_fastqc_input){
  as_tibble(read.table(multiqc_fastqc_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Filename,1,6)) %>%
  mutate(read = ifelse(grepl("R1", Filename), "read1", "read2")) %>%
  mutate(fastq_processing = gsub("_.*$","",substr(Sample, 8, length(Sample)))) %>%
  select(!c(Sample,File.type,Encoding)) %>%
  pivot_wider(
    names_from = c(read,fastq_processing),
    values_from = !c(library,read,fastq_processing))
}

fastqc = process_multiqc_fastqc(fastqc_input)
  as_tibble(read.table(fastqc_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Sample, 1, 6)) %>%
  mutate(bam_processing = gsub("_.*$","",substr(Sample, 8, length(Sample)))) %>%
  select(!c(Sample)) %>%
  pivot_wider(
    names_from = c(bam_processing),
    values_from = !c(library, bam_processing))

process_multiqc_samfile = function(multiqc_samfile){
  read_tsv(multiqc_samfile) %>% mutate(library = substr(Sample, 1, 6)) %>%
  mutate(bam_processing = gsub("_.*$","",gsub("lib..._","", Sample))) %>%
  select(!c(Sample)) %>%
  pivot_wider(
    names_from = c(bam_processing),
    values_from = !c(library, bam_processing))
}

samstats = process_multiqc_samfile(samstats_input)
flagstats = process_multiqc_samfile(flagstats_input)

deeptools_frag = read_tsv(deeptools_frag_input, col_names = c("frag_len","frag_count","file"), skip = 1) %>%
  filter(frag_len < 500) %>%
  mutate(library = substr(gsub("^.*lib", "lib", file), 1,6)) %>%
  mutate(frag_len = sub("^", "frag_len", frag_len)) %>%
  select(library, frag_len, frag_count) %>%
  pivot_wider(
    names_from = frag_len,
    values_from = frag_count)

picard = as_tibble(read.table(picard_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = Sample)

deeptools_cov = read_tsv(deeptools_cov_input, skip = 1) %>%
  pivot_longer(!c(`#'chr'`, `'start'`,`'end'`), names_to = "file", values_to = "cnt") %>%
  rename(chr = `#'chr'`,
         start = `'start'`,
         end = `'end'`) %>%
  mutate(library = substr(file, 2, 7)) %>%
  group_by(library) %>%
  summarise(
    mean_cov = mean(cnt),
    median_cov = median(cnt),
            )

readqc = fastqc %>%
  left_join(samstats, by = "library") %>%
  left_join(flagstats, by = "library") %>%
  left_join(deeptools_frag, by = "library") %>%
  left_join(picard, by = "library") %>%
  left_join(deeptools_cov, by = "library")

write.table(readqc, file = readqc_out_tbl, row.names = F, sep = '\t', quote = F)

all_frag_len = data.frame(frag_len = 1:500)

frag_len =
  readqc %>% select(starts_with("frag_len") | matches("library")) %>%
  pivot_longer(!library, names_to = "frag_len", values_to = "count") %>%
  mutate(frag_len = as.numeric(gsub("frag_len","",frag_len))) %>%
  mutate(count = as.numeric(count)) %>%
  pivot_wider(names_from = library, values_from = count) %>%
  right_join(all_frag_len) %>% arrange(frag_len) %>%
  replace(is.na(.), 0)

write_tsv(frag_len, file = frag_len_out_tbl)

#+end_src
*** Development
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
**** deepTools bamCoverage                                         :smk_rule:
:PROPERTIES:
:ID:       a750e80a-36bd-4498-8789-a0c9a076b484
:END:
- [[./workflow/reads.smk::rule cfdna_wgs_bamcoverage][Snakemake]]
  #+begin_src snakemake
# Make deeptools bamCoverage bedfile
rule frag_bamcoverage:
    benchmark: benchdir + "/{library}_frag_bamcoverage.benchmark.txt",
    input: frag_bams + "/{library}_filt.bam",
    log: logdir + "/{library}_frag_bamcoverage.log",
    output: qcdir + "/{library}_bamcoverage.bg",
    params:
        bin = "10000",
        blacklist = config["blklist"],
        script = "{cfdna_script_dir}/bamcoverage.sh",
        threads = frag_threads,
    shell:
        """
        {params.script} \
        {input} \
        {output} \
        {params.bin} \
        {params.blacklist} \
        {params.threads} &> {log}
        """
#+end_src
- [[file:scripts/bamcoverage.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/bamcoverage.sh
#!/usr/bin/env bash

in_bam=$1
bin=$3
blacklist=$4
threads=$5
out_bg=$2

bamCoverage \
    --bam $in_bam \
    --binSize $bin \
    --blackListFileName $blacklist \
    --effectiveGenomeSize 2913022398 \
    --extendReads \
    --ignoreDuplicates \
    --ignoreForNormalization chrX \
    --normalizeUsing RPGC \
    --numberOfProcessors $threads \
    --outFileFormat bedgraph \
    --outFileName $out_bg
#+end_src
**** Get fasta
#+begin_src snakemake
rule get_cfdna_wgs_fasta:
    log: f"{log_dir}/{{build}}_get_cfdna_wgs_fasta.log",
    output: f"{ref_dir}/{{build}}.fa",
    params:
        ftp = lambda wildcards: gene_build_map[wildcards.gene_build]['ftp'],
        script = f"{cfdna_script_dir}/get_cfdna_wgs_fasta.sh",
    shell:
        """
        {params.script} {params.ftp} {output} > {log} 2>&1
        """
#+end_src

#+begin_src bash :tangle ./scripts/get_cfdna_wgs_fasta.sh
#!/usr/bin/env bash
#+end_src
** cfDNA WGS CNA
:PROPERTIES:
:header-args:snakemake: :tangle ./workflows/cfdna_wgs_cna.smk
:END:
*** Preamble
:PROPERTIES:
:ID:       99b61b72-5f73-4f0c-b22b-45ae5049e1aa
:END:
#+begin_src snakemake

#########1#########2#########3#########4#########5#########6#########7#########8
#                                                                              #
#                   Snakefile for Analysis of Cell-free DNA                    #
#    Whole Genome Sequencing Copy Number Alteration and Fragmentomics          #
#                                                                              #
#########1#########2#########3#########4#########5#########6#########7#########8

#+end_src
*** Downsample all bam files to a consistent read pair count       :smk_rule:
:PROPERTIES:
:ID:       bc8e3589-2293-4029-b46c-0cbc025fef58
:END:

Downsample bam files on a per-directory basis

Note: The associated shell script will not result downsampled bams if sufficent reads do not exist in the original. Sufficent reads need to be confirmed externally.

#+begin_src snakemake
rule cfdna_wgs_downsample_bam:
    input: f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_filt.bam",
    log: f"{log_dir}/{{library}}_{{build}}_ds{{mil_reads}}_cfdna_wgs_downsample_bam.log",
    output: f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_ds{{mil_reads}}.bam",
    params:
        milreads = lambda wildcards: wildcards.mil_reads,
        script = f"{cfdna_script_dir}/downsample_bam.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        {input} \
        {params.milreads} &> {log}
        """
#+end_src

#+begin_src bash :tangle ./scripts/downsample_bam.sh
#!/usr/bin/env bash

in_bam=$1
milreads="$2"

reads=$(echo |awk -v var1=$milreads '{ print 1000000*var1 }')

out_bam=$(echo $in_bam | sed 's/_filt.bam/_ds10.bam/g')

## Calculate the sampling factor based on the intended number of reads:

FACTOR=$(samtools idxstats $in_bam | cut -f3 | awk -v COUNT=$reads 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

samtools view -@ 4 -s $FACTOR -b $in_bam > $out_bam

samtools index $out_bam
#+end_src
*** Filter cfDNA fragments by length range

Note: an index is needed for subsequent wig creation

#+begin_src snakemake
rule cfdna_wgs_frag_filt:
    input: f"{cfdna_wgs_dir}/bams/{{library}}_{{build}}_ds{{mil_reads}}.bam",
    log: f"{log_dir}/{{library}}_{{build}}_ds{{mil_reads}}_{{frag_distro}}_cfdna_wgs_frag_filt.log",
    params: script = f"{cfdna_script_dir}/wgs_frag_filt.sh",
    output:
        nohead = temp(f"{cfdna_wgs_dir}/frag_bams/{{library}}_{{build}}_ds{{mil_reads}}_frag{{frag_distro}}.nohead"),
        onlyhead = temp(f"{cfdna_wgs_dir}/frag_bams/{{library}}_{{build}}_ds{{mil_reads}}_frag{{frag_distro}}.onlyhead"),
        final = f"{cfdna_wgs_dir}/frag_bams/{{library}}_{{build}}_ds{{mil_reads}}_frag{{frag_distro}}.bam",
        index = f"{cfdna_wgs_dir}/frag_bams/{{library}}_{{build}}_ds{{mil_reads}}_frag{{frag_distro}}.bam.bai",
    shell:
        """
        frag_min=$(echo {wildcards.frag_distro} | sed -e "s/_.*$//g")
        frag_max=$(echo {wildcards.frag_distro} | sed -e "s/^.*_//g")
        {params.script} \
        {input} \
        {output.nohead} \
        $frag_min \
        $frag_max \
        4 \
        {output.onlyhead} \
        {output.final} &> {log}
        samtools index {output.final}
        """
#+end_src

#+begin_src bash :tangle ./scripts/wgs_frag_filt.sh
#!/usr/bin/env bash

#########################################
###   Filter Bam By Fragment Length   ###
#########################################

inbam="${1}"
nohead="${2}"
min="${3}"
max="${4}"
threads="${5}"
onlyhead="${6}"
outbam="${7}"

# Filter by absolute value of TLEN for each read
samtools view -@ $threads $inbam |
    awk -F'\t' -v upper="$max" 'sqrt($9*$9) < upper {print $0}' |
    awk -F'\t' -v lower="$min" 'sqrt($9*$9) > lower {print $0}'> $nohead

# Restore header
samtools view -@ $threads --header-only $inbam > $onlyhead


cat $onlyhead $nohead |
    samtools view -@ $threads --bam /dev/stdin |
    samtools sort -@ $threads -o $outbam /dev/stdin


#+end_src
*** Convert fragment length filtered bams to wigs
#+begin_src snakemake
rule bam_to_wig:
    input: f"{cfdna_wgs_dir}/frag_bams/{{library}}_{{build}}_ds{{mil_reads}}_frag{{frag_distro}}.bam",
    log: f"{log_dir}/{{library}}_{{build}}_{{mil_reads}}_{{frag_distro}}_cfdna_bam_to_wig.log",
    output: ensure(f"{cfdna_wgs_dir}/cna/wigs/{{library}}_{{build}}_ds{{mil_reads}}_frag{{frag_distro}}.wig", non_empty=True),
    params:
        chrs = "chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chrX,chrY",
        out_dir = f"{cfdna_wgs_dir}/cna/wigs",
    shell:
        """
        mkdir -p {params.out_dir} && readCounter --window 1000000 --quality 20 --chromosome {params.chrs} {input} > {output}
        """

#+end_src
*** Run ichorCNA
#+begin_src snakemake
rule cfdna_wgs_ichor:
    input: f"{cfdna_wgs_dir}/cna/wigs/{{library}}_{{build}}_ds{{mil_reads}}_frag{{frag_distro}}.wig",
    log: f"{log_dir}/{{library}}_{{build}}_ds{{mil_reads}}_frag{{frag_distro}}_cfdna_wgs_ichor.log",
    output: f"{cfdna_wgs_dir}/cna/ichor_nopon/{{library}}_{{build}}_ds{{mil_reads}}_frag{{frag_distro}}.RData",
    params: out_dir = f"{cfdna_wgs_dir}/cna/ichor_nopon",
    shell:
        """
        Rscript /opt/miniconda3/envs/mpnst/bin/runIchorCNA.R \
        --id {wildcards.library}_frag{wildcards.frag_distro} \
        --WIG {input} \
        --gcWig /opt/miniconda3/envs/mpnst/lib/R/library/ichorCNA/extdata/gc_hg38_1000kb.wig \
        --mapWig /opt/miniconda3/envs/mpnst/lib/R/library/ichorCNA/extdata/map_hg38_1000kb.wig \
        --centromere /opt/miniconda3/envs/mpnst/lib/R/library/ichorCNA/extdata/GRCh38.GCA_000001405.2_centromere_acen.txt \
        --normal "c(0.95, 0.99, 0.995, 0.999)" \
        --ploidy "c(2)" \
        --maxCN 3 \
        --estimateScPrevalence FALSE \
        --scStates "c()" \
        --outDir {params.out_dir}
        """

#+end_src

*** Development
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
***** Functions
:PROPERTIES:
:ID:       60269b14-e3ec-42f7-8a07-880252f3e415
:END:
#+begin_src snakemake
libraries = pd.read_table(config["data_dir"] + "/inputs/libraries.tsv")

readable = []
for x in libraries.file:
    readable.append(os.access(x, os.R_OK))
libraries['readable']=readable

cfdna_libraries = libraries
cfdna_libraries = cfdna_libraries[cfdna_libraries.library_type == "wgs"]
cfdna_libraries = cfdna_libraries[cfdna_libraries.isolation_type == "cfdna"]
cfdna_libraries = cfdna_libraries[cfdna_libraries.readable == True]

library_indict = cfdna_libraries["library"].tolist()
file_indict = cfdna_libraries["file"].tolist()
lib_dict = dict(zip(library_indict, file_indict))

FRAG_LIBS = list(lib_dict.keys())

cna_libraries = pd.read_table(config["data_dir"] + "/inputs/cna_libraries.tsv")

readable = []
for x in cna_libraries.bam_file:
    readable.append(os.access(x, os.R_OK))
cna_libraries['readable']=readable

cna_libraries = cna_libraries[cna_libraries.readable == True]

library_indict = cna_libraries["library"].tolist()
file_indict = cna_libraries["bam_file"].tolist()
lib_dict = dict(zip(library_indict, file_indict))

CNA_WGS_LIBRARIES = list(lib_dict.keys())


#+end_src
***** All rule
:PROPERTIES:
:ID:       e35a6255-dbca-42ed-a91f-67efd791b7ee
:END:
#+begin_src snakemake
rule all:
    input:
# # From this snakefile:
#         # frag_symlink:
#         expand(frag_cna_in_bams +
#                "/{library}.bam",
#                library = lib_dict.keys()),
# # From cna.smk
#         # cna_frag_filt:
#         expand(frag_cna_frag_bams +
#                "/{library}_frag{frag_distro}.bam",
#                library = CNA_WGS_LIBRARIES,
#                frag_distro = FRAG_DISTROS),
#         # bam_to_wig:
#         expand(frag_cna_wigs +
#                "/{library}_frag{frag_distro}.wig",
#                library = CNA_WGS_LIBRARIES,
#                frag_distro = FRAG_DISTROS),
#         # ichor_nopon:
#         expand(frag_cna_ichor_nopon +
#                "/{library}_frag{frag_distro}.cna.seg",
#                library = CNA_WGS_LIBRARIES,
#                frag_distro = FRAG_DISTROS),
# From frag.smk
        # make_gc_map_bind:
        refdir + "/keep_5mb.bed",
        # filt_bam_to_frag_bed:
        expand(frag_frag_beds +
               "/{library}_filt.bed",
               library = CNA_WGS_LIBRARIES),
        # # gc_distro:
        # expand(frag_frag_gc_distros +
        #        "/{library}_gc_distro.csv",
        #        library = CNA_WGS_LIBRARIES),
        # # healthy_gc:
        # frag_frag_gc_distros + "/healthy_med.rds",
        # #
        # expand(frag_frag_beds +
        #        "/{library}_sampled_frag.bed",
        #       library = CNA_WGS_LIBRARIES),
        # expand(frag_frag_beds) /
        #        "{library}_norm_{length}.bed",
        #        library = CNA_WGS_LIBRARIES,
        #        length = ["short", "long"]),
        expand(frag_frag_counts +
               "/{library}_cnt_{length}.tmp",
               library = CNA_WGS_LIBRARIES,
               length = ["short", "long"]),
        frag_frag + "/frag_counts.tsv",
        #
        # unit_cent_sd:
        frag_frag + "/ratios.tsv",
#+end_src

***** Filter downsampled bams to set fragment length distributions
:PROPERTIES:
:ID:       b49bd73f-5d02-4c8c-984d-a6631fc48c12
:END:
#+begin_src snakemake
rule frag_filt:
    input:
        main = frag_bams + "/{library}_ds{downsample}.bam",
        check = logdir + "/{library}_{downsample}_made",
    output:
        nohead = temp(frag_bams + "/{library}_ds{downsample}_frag{frag_distro}.nohead"),
        onlyhead = temp(frag_bams + "/{library}_ds{downsample}_frag{frag_distro}.only"),
        final = frag_bams + "/{library}_ds{downsample}_frag{frag_distro}.bam",
    params:
        script = "{frag_script_dir}/frag_filt.sh",
        threads = frag_threads,
    shell:
        """
        frag_min=$(echo {wildcards.frag_distro} | sed -e "s/_.*$//g")
        frag_max=$(echo {wildcards.frag_distro} | sed -e "s/^.*_//g")
        {params.script} \
        {input.main} \
        {output.nohead} \
        $frag_min \
        $frag_max \
        {config[threads]} \
        {output.onlyhead} \
        {output.final}
        """
#+end_src
- [[file:./scripts/frag_filt.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/frag_filt.sh
#!/usr/bin/env bash

# Steps
## Filter by absolute value of TLEN for each read
sambamba view -t $5 $1 | awk -F'\t' -v upper="$4" 'sqrt($9*$9) < upper {print $0}' | awk -F'\t' -v lower="$3" 'sqrt($9*$9) > lower {print $0}'> $2

## Restore header
sambamba view -H $1 > $6

cat $6 $2 | sambamba view -t 4 -S -f bam /dev/stdin | sambamba sort -t 4 -o $7 /dev/stdin


#+end_src
***** Setup conditional execution of downsampled bams
:PROPERTIES:
:ID:       6a626e68-c7b3-4702-86d1-5495425c9bf2
:END:
#+begin_src snakemake

# If downsample occured, then write filename into this per-library log, else leave the log file blank
rule log_dowsample:
    input: logdir + "/{library}_{downsample}_downsample.done",
    output: logdir + "/{library}_{downsample}_made",
    params:
        bamdir = frag_bams,
    shell:
        """
        dspath={params.bamdir}/{wildcards.library}_ds{wildcards.downsample}.bam
        if [ -f $dspath ]; then echo "$dspath"  > {output}; else touch {output}; fi
        """

# Use the downsampled bam logs to make a single text file of conditionally executed final targets.
# Specifically in this example, log text lines are in the form
# frag_bams + "/{library}_ds{downsample}_frag90_150.bam" to setup conditional execution of fragment filtering ONLY on downsampled bams
# Note alternative delimiter "~" to sed allows frag_wigs as param

checkpoint ds_cond_target_list:
    input: expand(logdir + "/{library}_{downsample}_made", library = FRAG_LIBS, downsample = DOWNSAMPLE),
    output: logdir + "/ds_final_targets",
    params:
        outdir = frag_bams,
        frag_distro=config["frag_distro"]
    shell:
        """
        if [ -f {output} ]; then rm {output}; fi
        cat {input} > {output}
        sed -i 's~^.*lib~{params.outdir}/lib~g' {output}
        sed -i 's/.bam$/_frag{params.frag_distro}.bam/g' {output}
        """

# Function jsut pulls the final target names out of ds_final_targets
def get_ds_targets(wildcards):
    with open(checkpoints.ds_cond_target_list.get(**wildcards).output[0], "r") as f:
      non_empty_files = [l.strip() for l in f.readlines()]
    return non_empty_files

# This rule allows execution of rules which will generate the conditional targets in ds_cond_target_list
rule make_ds_targets:
    input:
        get_ds_targets
    output: logdir + "/aggregate_output"
    run:
        with open(output[0], "w") as f:
            f.write("\n".join(input))
#+end_src

** [[file:config/frag_env.yaml][Fragmentomics environment YAML]]
:PROPERTIES:
:ID:       4e606db9-72e7-4c62-acdc-224c34e4bc3d
:END:
#+begin_src fundamental :tangle ./config/frag_env.yaml
name: frag
channels:
  - conda-forge
  - bioconda

dependencies:
  - bwa
  - r-tidyverse
  - samtools
  - snakemake
#+end_src
** [[file:config/int_test.yaml][Snakemake configuration YAML]]
:PROPERTIES:
:header-args:bash: :tangle ./config/int_test.yaml
:ID:       7b9f7d71-ef63-4980-90ce-21903eacbef7
:END:
#+begin_src bash

##############################
###   Configuration Yaml   ###
##############################

###   Parameters Intended To Be Common Across Workflows    ###

blklist: "/mnt/ris/szymanski/Active/test/inputs/hg38-blacklist.v2.bed.gz"
data_dir: "/mnt/ris/szymanski/Active/test"
genome_fasta: "/mnt/ris/szymanski/Active/test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"
threads: 4

###   Unique properties from this repo   ###

frag_repo: "/home/jszymanski/repos/cfdna-wgs"

frag_distro: "90_150"

gc5mb: "test/inputs/gc5mb.bed"

#+end_src
** End motifs
*** Sample 5' motifs from a bam file

**** Shell
#+begin_src bash :tangle ./scripts/sample_motifs.sh
in_bam="${1}"
in_fasta="${2}"
n_motif="${3}"
n_reads="${4}"
seed="${5}"
threads="${6}"
out_merged="${7}"

main(){
    forward_motif \
        $in_bam \
        $seed \
        $threads \
        $n_reads \
        $in_fasta \
        $n_motif > $out_merged
    reverse_motif \
        $in_bam \
        $seed \
        $threads \
        $n_reads \
        $in_fasta \
        $n_motif >> $out_merged
}

#########1#########2#########3#########4#########5#########6#########7#########8
forward_motif(){
    #
    local in_bam="${1}"
    local seed="${2}"
    local threads="${3}"
    local n_reads="${4}"
    local in_fasta="${5}"
    local n_motif="${6}"
    #
    # Calculate the samtools sampling factor based on the intended number of
    # reads. This will be 2x the n_read input plus some margin of error as
    # the next set will only to forward reads.
    f_reads=$(( 3*$n_reads ))
    factor=$(samtools idxstats $in_bam |
                 cut -f3 |
                 awk -v nreads=$f_reads 'BEGIN {total=0} {total += $1} END {print nreads/total}')
    #
    # Take first read in mapped, paired, with normal FS orientation.
    # View perfect matching reads (for BWA), first in pair.
    samtools view \
             --with-header \
             --min-MQ 60 \
             --require-flags 65 \
             --subsample $factor \
             --subsample-seed $seed \
             --threads $threads $in_bam |
        # Fetch reference for n reads
        bedtools bamtobed -i stdin | head -n $n_reads |
        bedtools getfasta -bed stdin -fi $in_fasta |
        # Sed magic to extract motifs from fasta
        sed "1d; n; d" | sed -E "s/(.{$n_motif}).*/\1/"
}

#########1#########2#########3#########4#########5#########6#########7#########8
reverse_motif(){
    #
    local in_bam="${1}"
    local seed="${2}"
    local threads="${3}"
    local n_reads="${4}"
    local in_fasta="${5}"
    local n_motif="${6}"
    #
    # Calculate the samtools sampling factor based on the intended number of
    # reads. This will be 2x the n_read input plus some margin of error as
    # the next set will only to forward reads.
    f_reads=$(( 3*$n_reads ))
    factor=$(samtools idxstats $in_bam |
                 cut -f3 |
                 awk -v nreads=$f_reads 'BEGIN {total=0} {total += $1} END {print nreads/total}')
    #
    # Take SECOND read in mapped, paired, with normal FS orientation.
    # View perfect matching reads (for BWA).
    samtools view \
             --with-header \
             --min-MQ 60 \
             --require-flags 129 \
             --subsample $factor \
             --subsample-seed $seed \
             --threads $threads $in_bam |
        # Fetch reference for n reads
        bedtools bamtobed -i stdin | head -n $n_reads |
        bedtools getfasta -bed stdin -fi $in_fasta |
        # Sed magic to extract motifs from fasta
        sed "1d; n; d" | sed -E "s/.*(.{$n_motif})/\1/" |
        # Generate reverse compliment
        tr ACGT TGCA | rev
}

main "$@"
#+end_src
*** Create matrix of all motif sampling from 5' sampling
#+begin_src R :tangle ./scripts/motif.mat.R
#!/usr/bin/env Rscript

########################################
###   Make End Motif Single Matrix   ###
########################################

args = commandArgs(trailingOnly = TRUE)
motif_str = args[1]
motif_tsv = args[2]

# Load required packages, data, and functions
library(tidyverse)

# Define possible 4-mer motifs
possible_motifs =
  expand.grid(rep(list(c('A', 'G', 'T', 'C')), 4)) %>%
  as_tibble() %>%
  mutate(motif = paste0(Var1,Var2,Var3,Var4)) %>%
  select(motif) %>% arrange(motif)
possible_motifs

# Define motif files list
#motif_str = "~/mpnst/analysis/frag/motifs/lib005_motifs.txt ~/mpnst/analysis/frag/motifs/lib507_motifs.txt"
(motif_files = strsplit(motif_str, " ")[[1]])
(names(motif_files) = substr(gsub("^.*lib","lib",motif_files), 1, 6))


#(motif_files = list.files(motif_samples_dir, full.names = TRUE, pattern = "^lib.*motifs.txt"))
#(names(motif_files)=substr(list.files(motif_samples_dir, pattern = "^lib.*motifs.txt"),1,6))

# Make per-libary motif frequencies
ingest_motif = function(motif_file){
  read_tsv(motif_file,
           col_names = c("motif")) %>%
    group_by(motif) %>%
    summarise(count = n()) %>%
    mutate(fract = count/sum(count)) %>%
    select(motif, fract)
}

motif_tibs = lapply(motif_files, ingest_motif)

# Make single matrix tsv
motifs = bind_rows(motif_tibs, .id = "library") %>% pivot_wider(names_from = library, values_from = fract) %>% filter(motif %in% possible_motifs$motif)

motifs %>% write_tsv(., motif_tsv)

#+end_src

*** F-profile NMF
***
** Fragmentomics                                                        :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/frag.smk
:ID:       a72c2025-a18a-45a4-b4e7-efdafeaee80e
:END:
*** Preamble
:PROPERTIES:
:ID:       325d69b2-efcf-4695-a222-c9428a7d3d39
:END:
#+begin_src snakemake

#########1#########2#########3#########4#########5#########6#########7#########8
#                                                                              #
#     Fragmentomic Analysis of Cell-free DNA Whole Genome Sequencing           #
#                                                                              #
#########1#########2#########3#########4#########5#########6#########7#########8

#+end_src
*** Make GC and mappability restricted bins
:PROPERTIES:
:ID:       8976ec06-7aa2-40be-adfe-ee5a9df4feaf
:END:
- Snakemake
  #+begin_src snakemake
rule make_gc_map_bind:
    container: frag_container,
    input:
        gc5mb = config["gc5mb"],
        blklist = config["blklist"],
    log: logdir + "/make_gc_map_bind.log",
    output: refdir + "/keep_5mb.bed",
    params:
        script = "{frag_script_dir}/make_gc_map_bind.sh",
    shell:
        """
        {params.script} \
        {input.gc5mb} \
        {input.blklist} \
        {output} &> {log}
        """
#+end_src
- Shell script
  #+begin_src bash :tangle ./scripts/make_gc_map_bind.sh
gc5mb="${1}"
blklist="${2}"
keep="${3}"

bedtools intersect -a $gc5mb -b $blklist -v -wa |
    grep -v _ |
    awk '{ if ($4 >= 0.3) print $0 }' > $keep
#+end_src
*** Make bedfile from filtered bam                                 :smk_rule:
:PROPERTIES:
:ID:       8699bab9-85c6-4903-8f8b-35d6dca5a0d5
:END:
- error may be multimappers https://www.biostars.org/p/55149/
  - https://bioinformatics.stackexchange.com/questions/508/obtaining-uniquely-mapped-reads-from-bwa-mem-alignment
- [[./workflow/frag.smk::rule cfdna_wgs_filt_bam_to_frag_bed][Snakemake]]
  #+begin_src snakemake
# Make a bed file from filtered bam
rule filt_bam_to_frag_bed:
    benchmark: benchdir + "/{library}_filt_bam_to_frag_bed.benchmark.txt",
    container: frag_container,
    input: frag_bams + "/{library}_filt.bam",
    log: logdir + "/{library}_filt_bam_to_frag_bed.log",
    output: frag_frag_beds + "/{library}_filt.bed",
    params:
        fasta = genome_fasta,
        script = "{frag_script_dir}/filt_bam_to_frag_bed.sh",
        threads = frag_threads,
    shell:
        """
        {params.script} \
	{input} \
        {params.fasta} \
        {params.threads} \
        {output}
        """
#+end_src
- [[file:./scripts/filt_bam_to_frag_bed.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/filt_bam_to_frag_bed.sh
#!/usr/bin/env bash

# Snakemake variables
input_bam="$1"
params_fasta="$2"
threads="${3}"
output_frag_bed="$4"

# Function
bam_to_frag(){
    # Ensure name-sorted bam file
    samtools sort -@ $threads -n -o - $1 |
    samtools fixmate -@ $threads -m -r - - |
    # Make bedpe
    bedtools bamtobed -bedpe -i - |
    # Filter any potential non-standard alignments
    awk '$1==$4 {print $0}' | awk '$2 < $6 {print $0}' |
    # Create full-fragment bed file
    awk -v OFS='\t' '{print $1,$2,$6}' |
    # Annotate with GC content and fragment length
    bedtools nuc -fi $2 -bed stdin |
    # Convert back to standard bed with additional columns
    awk -v OFS='\t' '{print $1,$2,$3,$5,$12}' |
    sed '1d' > $3
}

# Run command
bam_to_frag $input_bam \
            $params_fasta \
            $output_frag_bed

#+end_src
*** Make GC distributions                                          :smk_rule:
:PROPERTIES:
:ID:       9dd802f2-b7bc-44f6-a6a4-0c70da5ce763
:END:
- [[./workflow/frag.smk::rule cfdna_wgs_gc_distro][Snakemake]]
  #+begin_src snakemake
# Make GC distributions
rule gc_distro:
    benchmark: benchdir + "/{library}_frag_gc_distro.benchmark.txt",
    container: frag_container,
    input: frag_frag_beds + "/{library}_filt.bed",
    log: logdir + "/{library}_frag_gc_distro.log",
    output: frag_frag_gc_distros + "/{library}_gc_distro.csv",
    params:
        script = "{frag_script_dir}/gc_distro.R",
    shell:
        """
        Rscript {params.script} \
        {input} \
        {output} \
        > {log} 2>&1
        """
#+end_src
- [[file:./scripts/gc_distro.R][Rscript]]
  #+begin_src R :tangle ./scripts/gc_distro.R
#!/usr/bin/env Rscript
args = commandArgs(trailingOnly = TRUE)
bed_file = args[1]
distro_file = args[2]

library(tidyverse)

# Read in modified bed
bed = read.table(bed_file, sep = '\t')
names(bed) = c("chr","start","end","gc_raw","len")

# Generate distribution csv
distro =
  bed %>%
  # Round GC
  mutate(gc_strata = round(gc_raw, 2)) %>%
  # Count frags per strata
  count(gc_strata) %>%
  # Get fraction frags
  mutate(fract_frags = n/sum(n)) %>% mutate(library_id = gsub("_frag.bed", "", gsub("^.*lib", "lib", bed_file))) %>%
  select(library_id,gc_strata,fract_frags) %>%
  write.csv(file = distro_file, row.names = F)

#+end_src
*** Make healthy GC distributions summary file                     :smk_rule:
:PROPERTIES:
:ID:       8b87de70-1d6c-4d59-b15a-98e1acae6073
:END:
- [[./workflow/frag.smk::rule cfdna_wgs_healthy_gc][Snakemake]]
  #+begin_src snakemake
# Make healthy GC distributions summary file
rule healthy_gc:
    benchmark: benchdir + "/frag_healthy_gc.benchmark.txt",
    container: frag_container,
    input: expand(frag_frag_gc_distros + "/{library}_gc_distro.csv", library = FRAG_HEALTHY_LIBRARIES),
    log: logdir + "/frag_healthy_gc.log",
    output: frag_frag_gc_distros + "/healthy_med.rds",
    params:
        distro_dir = frag_frag_gc_distros,
        script = "{frag_script_dir}/healthy_gc.R",
    shell:
        """
        Rscript {params.script} \
        {params.distro_dir} \
        "{input}" \
        {output} > {log} 2>&1
        """
#+end_src
- [[file:./scripts/healthy_gc.R][Rscript]]
  #+begin_src R :tangle ./scripts/healthy_gc.R
#!/usr/bin/env Rscript
args = commandArgs(trailingOnly = TRUE)
distro_dir = args[1]
healthy_libs_str = args[2]
healthy_med_file = args[3]

library(tidyverse)

healthy_libs_distros = unlist(strsplit(healthy_libs_str, " "))

read_in_gc = function(gc_csv){
  read.csv(gc_csv, header = T)
}

healthy_list = lapply(healthy_libs_distros, read_in_gc)

# Bind
healthy_all = do.call(rbind, healthy_list)

# Summarize
healthy_med =
  healthy_all %>%
  group_by(gc_strata) %>%
  summarise(med_frag_fract = median(fract_frags))

# Save
saveRDS(healthy_med, file = healthy_med_file)
#+end_src
*** Sample fragments by healthy GC proportions                     :smk_rule:
:PROPERTIES:
:ID:       0e6e2eae-8a46-499e-a9fc-4168bdddeb09
:END:
- [[./workflow/frag.smk::rule cfdna_wgs_gc_sample][Snakemake]]
  #+begin_src snakemake
# Sample fragments by healthy GC proportions
rule frag_gc_sample:
    benchmark: benchdir + "/{library}_frag_gc_sample.benchmark.txt",
    container: frag_container,
    input:
        frag_bed = frag_frag_beds + "/{library}_filt.bed",
        healthy_med = frag_frag_gc_distros + "/healthy_med.rds",
    log: logdir + "/{library}_frag_gc_sample.log",
    output: frag_frag_beds + "/{library}_sampled_frag.bed",
    params:
        script = "{frag_script_dir}/gc_sample.R",
    shell:
        """
        Rscript {params.script} \
        {input.healthy_med} \
        {input.frag_bed} \
        {output} > {log} 2>&1
        """
#+end_src
- [[file:./scripts/gc_sample.R][Rscript]]
  #+begin_src R :tangle ./scripts/gc_sample.R
#!/usr/bin/env Rscript
args = commandArgs(trailingOnly = TRUE)
healthy_med = args[1]
frag_file = args[2]
sampled_file = args[3]

library(tidyverse)

healthy_fract = readRDS(healthy_med)
frag_file = read.table(frag_file, sep = '\t', header = F)

frag_bed = frag_file
names(frag_bed) = c("chr", "start", "end", "gc_raw", "len")

frag = frag_bed %>%
  # Round off the GC strata
  mutate(gc_strata = round(gc_raw, 2)) %>%
  # Join the median count of fragments per strata in healthies
  # Use this later as sampling weight
  left_join(healthy_fract, by = "gc_strata")

# Determine frags to sample by counts in strata for which healthies had highest count
stratatotake = frag$gc_strata[which.max(frag$med_frag_fract)]
fragsinmaxstrata = length(which(frag$gc_strata == stratatotake))
fragstotake = round(fragsinmaxstrata/stratatotake)

sampled = frag %>%
  filter(!is.na(med_frag_fract)) %>%
  slice_sample(., n = nrow(.), weight_by = med_frag_fract, replace = T) %>% select(chr, start, end, len, gc_strata)

write.table(sampled, sep = "\t", col.names = F, row.names = F, quote = F, file = sampled_file)
#+end_src
*** Sum fragments in genomic windows by length                     :smk_rule:
:PROPERTIES:
:ID:       d14368b2-2ab0-4f04-bf51-faf66971d3cf
:END:
- [[./workflow/frag.smk::rule cfdna_wgs_frag_window_sum][Snakemake]]
  #+begin_src snakemake

# Sum fragments in short and long length groups

rule frag_sum:
    benchmark: benchdir + "/{library}_frag_sum.benchmark.txt",
    container: frag_container,
    input: frag_frag_beds + "/{library}_sampled_frag.bed",
    log: logdir + "/{library}_frag_frag_window_sum.log",
    output:
        short = frag_frag_beds + "/{library}_norm_short.bed",
        long =  frag_frag_beds + "/{library}_norm_long.bed",
    params:
        script = "{frag_script_dir}/frag_window_sum.sh",
        threads = frag_threads,
    shell:
        """
        {params.script} \
        {input} \
        {output.short} {output.long} &> {log}
        """
#+end_src
- [[file:./scripts/frag_window_sum.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/frag_window_sum.sh
#!/usr/bin/env bash
input_frag="$1"
output_short="$2"
output_long="$3"

# Functions
make_short(){
    cat $1 | awk '{if ($4 >= 100 && $5 <= 150) print $0}' > $2
}

make_long(){
    cat $1 | awk '{if ($4 >= 151 && $5 <= 220) print $0}' > $2
}

# Run command
make_short $input_frag $output_short
make_long $input_frag $output_long

#+end_src
*** Count fragments intersecting windows                           :smk_rule:
:PROPERTIES:
:ID:       e94ee996-9543-4a29-9f85-b02469a3cbdb
:END:
- [[./workflow/frag.smk::rule cfdna_wgs_frag_window_int][Snakemake]]
  #+begin_src snakemake

# Count short and long fragments intersecting kept genomic windows

rule frag_window_count:
    benchmark: benchdir + "/{library}_frag_frag_window_int.benchmark.txt",
    container: frag_container,
    input:
        short = frag_frag_beds + "/{library}_norm_short.bed",
        long = frag_frag_beds + "/{library}_norm_long.bed",
        matbed = refdir + "/keep_5mb.bed",
    log: logdir + "/{library}_frag_frag_window_int.log",
    output:
        short = frag_frag_counts + "/{library}_cnt_short.tmp",
        long = frag_frag_counts + "/{library}_cnt_long.tmp",
    params:
        script = "{frag_script_dir}/frag_window_int.sh",
        threads = threads,
    shell:
        """
        {params.script} \
        {input.short} \
        {input.matbed} \
        {output.short}
        {params.script} \
        {input.long} \
        {input.matbed} \
        {output.long}
        """
#+end_src
- [[file:./scripts/frag_window_int.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/frag_window_int.sh
#!/usr/bin/env bash
input=$1
keep_bed=$2
output=$3

bedtools intersect -c \
             -a $keep_bed \
             -b $input > $output

#+end_src
*** Merge counts across length and library :smk_rule:
:PROPERTIES:
:ID:       97b6ff5e-0286-435c-adde-ceb3fdfcba65
:END:
- [[./workflow/frag.smk::rule cfdna_wgs_count_merge][Snakemake]]
  #+begin_src snakemake
# Merge short and long fragment counts by genomic poistion for all libraries
rule frag_count_merge:
    benchmark: benchdir + "/frag_count_merge.benchmark.txt",
    container: frag_container,
    input: expand(frag_frag_counts + "/{library}_cnt_{length}.tmp",  library = FRAG_LIBS, length = ["short","long"]),
    log: logdir + "/frag_count_merge.log",
    output:  frag_frag + "/frag_counts.tsv",
    params:
        counts_dir = frag_frag + "/counts",
        script = "{frag_script_dir}/count_merge.sh",
        threads = frag_threads,
    shell:
        """
        {params.script} \
        {params.counts_dir} \
        {output} &> {log}
        """
#+end_src
- [[file:./scripts/count_merge.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/count_merge.sh
# For unit testing
#counts_dir="/home/jeszyman/mpnst/analysis/cfdna-wgs/frag/counts"
#out_tsv="/home/jeszyman/mpnst/analysis/cfdna-wgs/frag/frag_counts.tsv"

# Define variables
counts_dir="${1}"
out_tsv="${2}"

# Remove the existing aggregate file if present
if [ -f $out_tsv ]; then rm $out_tsv; fi
#touch $out_tsv

# Make aggregate file
for file in ${counts_dir}/*;
do
    # Add file name to each line
    awk '{{print FILENAME (NF?"\t":"") $0}}' $file |
        # Modify file name to library id
        sed 's/^.*lib/lib/g' |
        sed 's/_.*_/\t/g' |
        # Cleanup "tmp"
        sed 's/.tmp//g' |
        # Send to output
        sed 's/\.bed//g' >> $out_tsv
done

# Add a header
sed -i  '1 i\library	len_class	chr	start	end	gc	count' $out_tsv

#+end_src

  #+begin_src bash
#!/usr/bin/env bash
output=$1
declare -a array2=$2

if [ -f $output ]; then \rm $output; fi

for file in ${array2[@]}; do
    awk '{{print FILENAME (NF?"\t":"") $0}}' $file |
        sed 's/^.*lib/lib/g' |
        sed 's/_.*_/\t/g' |
        # Cleanup "tmp"
        sed 's/.tmp//g' |
        sed 's/\.bed//g' >> $output
done

# Add a header
sed -i  '1 i\library	len_class	chr	start	end	count' $out_tsv

#+end_src
*** Make a zero-centered, unit SD fragment file
:PROPERTIES:
:ID:       d341f874-21d4-472e-9b5b-69436bcda5cd
:END:
- Snakemake
  #+begin_src snakemake
rule unit_cent_sd:
    benchmark: benchdir + "/unit_cent_sd.benchmark.txt",
    container: frag_container,
    input: frag_frag + "/frag_counts.tsv",
    log: logdir + "/unit_cent_sd.log",
    output: frag_frag + "/ratios.tsv",
    params:
        script = "{frag_script_dir}/make_ratios.R",
    shell:
        """
        Rscript {params.script} \
        {input} {output} > {log} 2>&1
        """
#+end_src
- Rscript
  #+begin_src R :tangle ./scripts/make_ratios.R
#!/usr/bin/env Rscript

# For unit testing
frags_tsv = "test/analysis/frag/frag/frag_counts.tsv"
ratios_tsv = "/home/jeszyman/mpnst/analysis/cfdna-wgs/frag/ratios.tsv"

args = commandArgs(trailingOnly = TRUE)
frags_tsv = args[1]
ratios_tsv = args[2]

# Load necessary packages
library(tidyverse)

# Load aggregate frag tsv
frags = read_tsv(frags_tsv)

# From per-position, per library short and long fragment counts, zero-centered fragment ratio
# See https://github.com/cancer-genomics/reproduce_lucas_wflow/blob/master/analysis/fig2a.Rmd

ratios =
  frags %>%
  mutate_at(vars(start, end, count), as.numeric) %>%
  # Put lib-bin short and long values on same row in order to make per-row ratios
  pivot_wider(names_from = len_class, values_from = count, values_fn = function(x) mean(x)) %>%
  mutate(fract = short/long) %>%
  select(library, chr, start, end, fract) %>%
  # Zero center by library
  group_by(library) %>%
  mutate(ratio.centered = scale(fract, scale=F)[,1])

write_tsv(ratios, file = ratios_tsv)
#+end_src
*** Reference :ref:
:PROPERTIES:
:ID:       b3b360ce-d696-4d55-b256-1eb70182b772
:END:
- Based on [[file:~/repos/biotools/biotools.org::*cfDNA fragmentomics][cfDNA fragmentomics]] cite:mathios2021
**** [[46270062-e3f4-46c9-9d71-5868376e495b][smk yas]]
:PROPERTIES:
:ID:       a0568e74-7619-4205-a707-b5c146c7901e
:END:
**** [[file:./workflow/frag.smk][Link to Snakefile]]
:PROPERTIES:
:ID:       58500b2b-bc08-465f-9c53-363a3d7b2b5f
:END:
*** Development :dev:
:PROPERTIES:
:header-args:snakemake: :tangle no
:ID:       bff6b6e6-6a68-4075-a5f1-008173bcb0f8
:END:
**** Ideas
:PROPERTIES:
:header-args:snakemake: :tangle no
:ID:       7bdb150e-957d-42a1-9fbe-b04da98851b4
:END:
** INPROCESS [[file:workflow/int_test.smk][Integration testing]] :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/int_test.smk
:ID:       2d0f6107-7d99-444e-82e4-0019db1835c3
:END:
*** Preamble
:PROPERTIES:
:ID:       f8c1cb6f-88fd-4b4d-939a-eaeb18a5c10f
:END:
#+begin_src snakemake

##################################################################
###   Integration testing snakefile for WGS cfDNA Processing   ###
##################################################################

#+end_src
*** Load packages
:PROPERTIES:
:ID:       d7718025-b503-4920-8d36-a9eb428a62d2
:END:
#+begin_src snakemake

import pandas as pd
import re
import numpy as np

#+end_src
*** Variable naming
:PROPERTIES:
:ID:       c4a0072f-09e5-454c-85e1-97d67f54e8a4
:END:
#+begin_src snakemake
# Values directly from configuration file
threads = config["threads"]
FRAG_DISTROS = config["frag_distro"]
frag_threads = config["threads"]
genome_fasta = config["genome_fasta"]
frag_repo = config["frag_repo"]

# Directory values derived from data_dir in configuration YAML
data_dir                   = config["data_dir"]
frag                 = data_dir + "/analysis/frag"
frag_bams            = data_dir + "/analysis/frag/bams"
frag_fastqs          = data_dir + "/analysis/frag/fastqs"
frag_frag            = data_dir + "/analysis/frag/frag"
frag_frag_beds       = data_dir + "/analysis/frag/frag/beds"
frag_frag_counts     = data_dir + "/analysis/frag/frag/counts"
frag_frag_gc_distros = data_dir + "/analysis/frag/frag/distros"
qcdir                     = data_dir + "/analysis/qc"
benchdir                  = data_dir + "/benchmark"
logdir                    = data_dir + "/logs"
refdir                    = data_dir + "/re"

frag_scriptdir = config["frag_repo"] +  "/scripts"

bwa_dir = "{data_dir}/ref/hg38"
fasta_base = "GCA_000001405.15_GRCh38_no_alt_analysis_set"
frag_script_dir = "{frag_repo}/scripts"
#+end_src
*** Functions, miscellaneous
:PROPERTIES:
:ID:       f36b260e-d78e-453b-bc2f-ca330edf1097
:END:
#+begin_src snakemake

#####################
###   Functions   ###
#####################

# Setup fragment sample name index as a python dictionary
frag_libs = pd.read_table("~/test/inputs/frag_libs.tsv")

lib_path = "{data_dir}/test/inputs"

# Ensure readable fastqs
readable = []
for x in lib_path + "/" + frag_libs["r1_basename"]:
    readable.append(os.access(x, os.R_OK))
frag_libs['readable']=readable
frag_libs = frag_libs[frag_libs.readable == True]

# Make the dictionary
FRAG_LIBS = frag_libs["library"].tolist()
frag_libs_file_indict = lib_path + "/" + frag_libs["r1_basename"]
frag_lib_dict = dict(zip(FRAG_LIBS, frag_libs_file_indict))

# Make  a list of healthy libraries
FRAG_HEALTHY_LIBRARIES = frag_libs[frag_libs['cohort'] == 'healthy']['library'].tolist()
#+end_src

*** All rule
:PROPERTIES:
:ID:       d8b9017c-8cde-4d5e-a492-9f8d21f5fa20
:END:
#+begin_src snakemake
data_dir="~/test"
rule all:
    input:
        expand("{data_dir}/analysis/frag/fastqs/{{library}}_raw_{{read}}.fastq.gz",
               library = list(frag_lib_dict.keys()),
               read = ["R1", "R2"]),
        #"{data_dir}/ref/{fasta_base}.sa",
        #"{data_dir}/ref/{fasta_base}.sa",
        #logdir + "/aggregate_output",
        #frag_frag + "/ratios.tsv",
        #qcdir + "/frag_read_qc.tsv",
        #qcdir + "/frag_frag_len.tsv",

#+end_src
*** Benchmark aggregation
:PROPERTIES:
:ID:       f4bb1ce5-c04c-4a38-95b5-c71d4fe19b91
:END:
#+begin_src snakemake

onsuccess:
    shell("""
        bash {frag_scriptdir}/agg_bench.sh {benchdir} {qcdir}/agg_bench.tsv
        """)
#+end_src
#+begin_src bash :tangle ./scripts/agg_bench.sh
# For unit testing
indir="test/benchmark"
output="test/analysis/qc/bench_agg.tsv"

if [ -f $output ]; then rm $output; fi

for file in $indir/*
do
    base=$(basename $file)
    cat $file | awk -v OFS='\t' -v var=$base 'NR>1 {print var,$0}' >> $output
done

sed -i '1i\process\tfloat_sec\trun_time\tmax_rss\tmax_vms\tmax_uss\tmax_pss\tio_in\tio_out\tmean_load\tcpu_time' $output

#+end_src

#+begin_src R
library(tidyverse)

bmk_in = read_tsv("~/repos/cfdna-wgs/test/analysis/qc/bench_agg.tsv")

bmk =
  bmk_in %>%
  mutate(process = gsub(".benchmark.txt", "", process)) %>%
  mutate(library = process) %>%
  mutate(library = ifelse(grepl("lib[0-9]{3}_", process),
                          sub("^.*lib(\\d{3}).*$", "lib\\1", process), "all_libs")) %>%
  mutate(process2 = process) %>%
  mutate(process = gsub("^lib..._","", process)) %>%
  rename(process_lib = process2)

find_outlier <- function(x) {
  return(x > quantile(x, .75) + 1.5*IQR(x))
}

bmk %>% mutate(outlier = ifelse(find_outlier(run_time), process_lib, NA)) %>%
  ggplot(.,aes(y=run_time)) +
  geom_boxplot() +
  geom_text(aes( y = run_time, x = .1,label=outlier), na.rm=TRUE, position = position_jitter())

bmk %>% mutate(outlier = ifelse(find_outlier(run_time), process_lib, NA)) %>%
  ggplot(.,aes( y = run_time)) +
  geom_boxplot() +
  geom_text(aes( y = run_time, x = .1,label=outlier), na.rm=TRUE, position = position_jitter())

#+end_src


*** Symlink input fastqs
:PROPERTIES:
:ID:       76e4f021-76a3-4ebb-a179-0f5837ee246b
:END:
#+begin_src snakemake
rule symlink_inputs:
    input:
        lambda wildcards: frag_lib_dict[wildcards.library],
    output:
        read1 = "{data_dir}/analysis/frag/fastqs/{{library}}_raw_R1.fastq.gz",
        read2 = "{data_dir}/analysis/frag/fastqs/{{library}}_raw_R2.fastq.gz",
    params:
        outdir = frag_fastqs,
        script = "{frag_script_dir}/symlink.sh",
    shell:
        """
        {params.script} \
        {input} \
        {output.read1} \
        {output.read2} \
        {params.outdir}
        """
#+end_src
#+begin_src bash :tangle ./scripts/symlink.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables
input_read1="${1}"
output_read1="${2}"
output_read2="${3}"
outdir="${4}"

mkdir -p $outdir

input_read2="$(echo $input_read1 | sed "s/_R1/_R2/g")"

ln -sf --relative ${input_read1} ${output_read1}
ln -sf --relative ${input_read2} ${output_read2}
#+end_src
*** Includes statements
:PROPERTIES:
:ID:       d78472f8-09a0-4546-b25a-72a78115c9c5
:END:
#+begin_src snakemake
include: frag_repo + "/workflow/frag_reads.smk"
#include: frag_repo + "/workflow/frag.smk"
#+end_src
** README
:PROPERTIES:
:export_file_name: ./README.md
:export_options: toc:nil ^:nil
:ID:       a94fc0ef-ea37-4ebb-9cce-4760fd637d15
:END:
*** Introduction
:PROPERTIES:
:ID:       0a8a24f4-7c5f-47d0-b057-026ebfddf4dc
:END:
This repository hosts a snakemake workflow for basic processing of whole-genome sequencing reads from cell-free DNA.

[[file:resources/int_test.png]]

*** Organization
:PROPERTIES:
:ID:       b826f026-70d1-480e-be6c-f829207124f0
:END:
Master branch of the repository contains most recent developments while stable versions are saved as terminal branches (/e.g./ stable.1.0.0).

Directory ~workflow~ contains two types of workflows- process-focused snakefiles (reads.smk, cna.smk, frag.smk) suitable for integration into another snakemake pipeline using the :include command, and the _int_test snakefile with examples of such integration using the repository test data.
*** Use
:PROPERTIES:
:ID:       b9eadaa0-d3ba-4c23-97a1-095f1cefcf6d
:END:
- All software needed for the pipeline is present within the associated docker container (see ~docker~ and https://hub.docker.com/repository/docker/jeszyman/frag/general).
- See the example configuration yaml ~config/int_test.yaml~ and wrapper workflow ~workflow/int_test.smk~ for necessary run conditions.
*** Changelog
:PROPERTIES:
:ID:       dbe230c9-b8a6-44f3-b9a1-67fd70f47895
:END:
- [2023-01-26 Thu] - Version 9.1.0: Repo cleanup
- [2023-01-26 Thu] - Version 9.0.0: Removed -f 3 flag for perfectly matched pairs in samtools filtering as the flag from BWA removes some fragments at a set max length. Added framework for benchmark analysis. Added conditional execution of downsampling. Removed (temporarily) final wig and ichor commands of CNA as these don't currently run correctly without full genome alignment, so can't be validated on test data. Added local documentation of cfdna-wgs dockerfile.
- [2023-01-21 Sat] - Version 8.0.0: Corrected rule filt_bam_to_frag_bed to fix mates of inputs, which seems to prevent errors in the bamtobed call. Frag_window_count now uses windows of consistent 5 Mb size, which are generated from rule make_gc_map_bind. Added a merged fragment counts file and zero-centered unit SD counts.
- [2022-12-07 Wed] - Version 7.0.0: Added copy number alteration and DELFI fragmentomics.
- [2022-10-17 Mon] - Version 6.0.0: Using fastp for read trimming (replaces trimmomatic). Simplified naming schema. Removed downsampling (will reinstate in later version).
- [2022-09-08 Thu] - Version 5.3.0: some minor name changes
- [2022-08-19 Fri] - Version 5.2.0 validated: Adds bamCoverage and plotCoverage from deeptools. Benchmarks BWA.
- [2022-08-09 Tue] - Version 5.1.0 validated: Added cfdna wgs-specific container for each rule, referenced to config
- [2022-08-05 Fri] - Version 5.0.0 validated: Added a symlink rule based on python dictionary. Added repo-specific output naming, added checks for sequence type and file readability to input tsv.
- [2022-06-27 Mon] - Version 4 validated. Further expanded read_qc.tsv table. Removed bam post-processing step and added a more expansive bam filtering step. Updated downsampling to work off filtered alignments.
- [2022-06-26 Sun] - Version 3.2 validated. Expanded the qc aggregate table and added some comments.
- [2022-06-24 Fri] - Validate version 3.1 which includes genome index build as a snakefile rule.
- [2022-06-24 Fri] - Validated version 3 with read number checkpoint for down-sampling.
- [2022-05-31 Tue] - Conforms to current biotools best practices.
- [2022-04-29 Fri] - Moved multiqc to integration testing as inputs are dependent on final sample labels. Integration testing works per this commit.
** Development                                                          :dev:
:PROPERTIES:
:header-args: :tangle no
:ID:       075335e3-4402-41c5-8e8a-739c399acedb
:END:
*** Version 9
:PROPERTIES:
:ID:       83223f42-7dcd-4687-a9e5-36ee2d34227f
:END:
- kill v7- not working for CNA
**** TODO [[id:f6717c79-64ce-4b16-b455-649df2ba20fd][Project stable version update]]
:PROPERTIES:
:ID:       69e7dd12-62a3-4313-8e98-b0c0c7ddf090
:END:
**** Make benchmarking table
:PROPERTIES:
:ID:       0e94013b-876e-4e8b-bdd3-2e818179fa58
:END:
- for file in ./*; do base=$(basename $file); $(str = tail -n1 $file); echo $base $str; done
*** Analysis of copy number alteration                                  :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/cna.smk
:ID:       4af6463f-f3b9-4fd1-b8d3-ebeccdf24b13
:END:
**** Preamble
:PROPERTIES:
:ID:       c68d847e-8ac5-4162-b041-3ab09927de6f
:END:
#+begin_src snakemake

#########1#########2#########3#########4#########5#########6#########7#########8
#                                                                              #
#                    Copy-number Alteration Analysis of                        #
#                  Cell-free DNA Whole Genome Sequencing                       #
#                                                                              #
#                                                                              #
#########1#########2#########3#########4#########5#########6#########7#########8

#+end_src
**** Convert bam to wig                                            :smk_rule:
:PROPERTIES:
:ID:       1d0a8925-f8b8-476b-87e2-4192259e3568
:END:
- [[./workflow/cna.smk::rule frag_bam_to_wig][Snakemake]]
  #+begin_src snakemake
# Use readCounter to create windowed wig from bam file
rule bam_to_wig:
    benchmark: benchdir + "/{library}_ds{downsample}_{frag_distro}_frag_bam_to_wig.benchmark.txt",
    container: frag_container,
    input: frag_bams + "/{library}_ds{downsample}_frag{frag_distro}.bam",
    log: logdir + "/{library}_ds{downsample}_{frag_distro}_frag_bam_to_wig.log",
    output: frag_wigs + "/{library}_ds{downsample}_frag{frag_distro}.wig",
    params:
        chrs = chrs,
        outdir = frag_wigs,
        script = "{frag_script_dir}/bam_to_wig.sh",
        threads = frag_threads,
    shell:
        """
        mkdir -p {params.outdir}
        /opt/hmmcopy_utils/bin/readCounter \
        --chromosome "{params.chrs}" \
        --quality 20 \
        --window 1000000 \
        {input} > {output}
        """
#+end_src
- [[file:./scripts/bam_to_wig.sh][Shell script]]
  #+begin_src bash :tangle ./scripts/bam_to_wig.sh
#!/usr/bin/env bash
input=$1
output=$2

        /opt/hmmcopy_utils/bin/readCounter --window 1000000 --quality 20 \
        --chromosome {params.chrs} \
        {input} > {output}

#+end_src
**** Run ichorCNA without a panel of normals                       :smk_rule:
:PROPERTIES:
:ID:       1bc62c09-2794-4f35-bad2-ea3723fe3ecf
:END:
- [[./workflow/cna.smk::rule frag_ichor_nopon][Snakemake]]
  #+begin_src snakemake
# Run ichorCNA without a panel of normals
rule ichor_nopon:
    input: frag_wigs + "/{library}_ds{downsample}_frag{frag_distro}.wig",
    output: frag_ichor_nopon + "/{library}_ds{downsample}_frag{frag_distro}.cna.seg",
    params:
        script = "{frag_script_dir}/MOD_runIchorCNA.R",
        out_dir = frag_ichor_nopon,
    container:
        frag_container,
    shell:
        """
        Rscript {params.script} \
         --id {wildcards.library}_frag{wildcards.frag_distro} \
         --WIG {input} \
         --gcWig /opt/ichorCNA/inst/extdata/gc_hg38_1000kb.wig \
         --mapWig /opt/ichorCNA/inst/extdata/map_hg38_1000kb.wig \
         --centromere /opt/ichorCNA/inst/extdata/GRCh38.GCA_000001405.2_centromere_acen.txt \
         --normal "c(0.95, 0.99, 0.995, 0.999)" \
         --ploidy "c(2)" \
         --maxCN 3 \
         --estimateScPrevalence FALSE \
         --scStates "c()" \
         --outDir {params.out_dir}
        """
#+end_src
- Rscript
  #+begin_src R :tangle ./scripts/MOD_runIchorCNA.R
# file:   ichorCNA.R
# authors: Gavin Ha, Ph.D.
#          Fred Hutch
# contact: <gha@fredhutch.org>
#
#         Justin Rhoades
#          Broad Institute
# contact: <rhoades@broadinstitute.org>

# ichorCNA: https://github.com/broadinstitute/ichorCNA
# date:   July 24, 2019
# description: Hidden Markov model (HMM) to analyze Ultra-low pass whole genome sequencing (ULP-WGS) data.
# This script is the main script to run the HMM.

library(optparse)

option_list <- list(
  make_option(c("--WIG"), type = "character", help = "Path to tumor WIG file. Required."),
  make_option(c("--NORMWIG"), type = "character", default=NULL, help = "Path to normal WIG file. Default: [%default]"),
  make_option(c("--gcWig"), type = "character", help = "Path to GC-content WIG file; Required"),
  make_option(c("--mapWig"), type = "character", default=NULL, help = "Path to mappability score WIG file. Default: [%default]"),
  make_option(c("--normalPanel"), type="character", default=NULL, help="Median corrected depth from panel of normals. Default: [%default]"),
  make_option(c("--exons.bed"), type = "character", default=NULL, help = "Path to bed file containing exon regions. Default: [%default]"),
  make_option(c("--id"), type = "character", default="test", help = "Patient ID. Default: [%default]"),
  make_option(c("--centromere"), type="character", default=NULL, help = "File containing Centromere locations; if not provided then will use hg19 version from ichorCNA package. Default: [%default]"),
  make_option(c("--minMapScore"), type = "numeric", default=0.9, help="Include bins with a minimum mappability score of this value. Default: [%default]."),
  make_option(c("--rmCentromereFlankLength"), type="numeric", default=1e5, help="Length of region flanking centromere to remove. Default: [%default]"),
  make_option(c("--normal"), type="character", default="0.5", help = "Initial normal contamination; can be more than one value if additional normal initializations are desired. Default: [%default]"),
  make_option(c("--scStates"), type="character", default="NULL", help = "Subclonal states to consider. Default: [%default]"),
  make_option(c("--coverage"), type="numeric", default=NULL, help = "PICARD sequencing coverage. Default: [%default]"),
  make_option(c("--lambda"), type="character", default="NULL", help="Initial Student's t precision; must contain 4 values (e.g. c(1500,1500,1500,1500)); if not provided then will automatically use based on variance of data. Default: [%default]"),
  make_option(c("--lambdaScaleHyperParam"), type="numeric", default=3, help="Hyperparameter (scale) for Gamma prior on Student's-t precision. Default: [%default]"),
  #	make_option(c("--kappa"), type="character", default=50, help="Initial state distribution"),
  make_option(c("--ploidy"), type="character", default="2", help = "Initial tumour ploidy; can be more than one value if additional ploidy initializations are desired. Default: [%default]"),
  make_option(c("--maxCN"), type="numeric", default=7, help = "Total clonal CN states. Default: [%default]"),
  make_option(c("--estimateNormal"), type="logical", default=TRUE, help = "Estimate normal. Default: [%default]"),
  make_option(c("--estimateScPrevalence"), type="logical", default=TRUE, help = "Estimate subclonal prevalence. Default: [%default]"),
  make_option(c("--estimatePloidy"), type="logical", default=TRUE, help = "Estimate tumour ploidy. Default: [%default]"),
  make_option(c("--maxFracCNASubclone"), type="numeric", default=0.7, help="Exclude solutions with fraction of subclonal events greater than this value. Default: [%default]"),
  make_option(c("--maxFracGenomeSubclone"), type="numeric", default=0.5, help="Exclude solutions with subclonal genome fraction greater than this value. Default: [%default]"),
  make_option(c("--minSegmentBins"), type="numeric", default=50, help="Minimum number of bins for largest segment threshold required to estimate tumor fraction; if below this threshold, then will be assigned zero tumor fraction."),
  make_option(c("--altFracThreshold"), type="numeric", default=0.05, help="Minimum proportion of bins altered required to estimate tumor fraction; if below this threshold, then will be assigned zero tumor fraction. Default: [%default]"),
  make_option(c("--chrNormalize"), type="character", default="c(1:22)", help = "Specify chromosomes to normalize GC/mappability biases. Default: [%default]"),
  make_option(c("--chrTrain"), type="character", default="c(1:22)", help = "Specify chromosomes to estimate params. Default: [%default]"),
  make_option(c("--chrs"), type="character", default="c(1:22,\"X\")", help = "Specify chromosomes to analyze. Default: [%default]"),
  make_option(c("--genomeBuild"), type="character", default="hg19", help="Geome build. Default: [%default]"),
  make_option(c("--genomeStyle"), type = "character", default = "NCBI", help = "NCBI or UCSC chromosome naming convention; use UCSC if desired output is to have \"chr\" string. [Default: %default]"),
  make_option(c("--normalizeMaleX"), type="logical", default=TRUE, help = "If male, then normalize chrX by median. Default: [%default]"),
  make_option(c("--minTumFracToCorrect"), type="numeric", default=0.1, help = "Tumor-fraction correction of bin and segment-level CNA if sample has minimum estimated tumor fraction. [Default: %default]"),
  make_option(c("--fracReadsInChrYForMale"), type="numeric", default=0.001, help = "Threshold for fraction of reads in chrY to assign as male. Default: [%default]"),
  make_option(c("--includeHOMD"), type="logical", default=FALSE, help="If FALSE, then exclude HOMD state. Useful when using large bins (e.g. 1Mb). Default: [%default]"),
  make_option(c("--txnE"), type="numeric", default=0.9999999, help = "Self-transition probability. Increase to decrease number of segments. Default: [%default]"),
  make_option(c("--txnStrength"), type="numeric", default=1e7, help = "Transition pseudo-counts. Exponent should be the same as the number of decimal places of --txnE. Default: [%default]"),
  make_option(c("--plotFileType"), type="character", default="pd", help = "File format for output plots. Default: [%default]"),
	make_option(c("--plotYLim"), type="character", default="c(-2,2)", help = "ylim to use for chromosome plots. Default: [%default]"),
  make_option(c("--outDir"), type="character", default="./", help = "Output Directory. Default: [%default]"),
  make_option(c("--libdir"), type = "character", default=NULL, help = "Script library path. Usually exclude this argument unless custom modifications have been made to the ichorCNA R package code and the user would like to source those R files. Default: [%default]")
)
parseobj <- OptionParser(option_list=option_list)
opt <- parse_args(parseobj)
print(opt)
options(scipen=0, stringsAsFactors=F)

library(HMMcopy)
library(GenomicRanges)
library(GenomeInfoDb)
options(stringsAsFactors=FALSE)
options(bitmapType='cairo')

patientID <- opt$id
tumour_file <- opt$WIG
normal_file <- opt$NORMWIG
gcWig <- opt$gcWig
mapWig <- opt$mapWig
normal_panel <- opt$normalPanel
exons.bed <- opt$exons.bed  # "0" if none specified
centromere <- opt$centromere
minMapScore <- opt$minMapScore
flankLength <- opt$rmCentromereFlankLength
normal <- eval(parse(text = opt$normal))
scStates <- eval(parse(text = opt$scStates))
lambda <- eval(parse(text = opt$lambda))
lambdaScaleHyperParam <- opt$lambdaScaleHyperParam
estimateNormal <- opt$estimateNormal
estimatePloidy <- opt$estimatePloidy
estimateScPrevalence <- opt$estimateScPrevalence
maxFracCNASubclone <- opt$maxFracCNASubclone
maxFracGenomeSubclone <- opt$maxFracGenomeSubclone
minSegmentBins <- opt$minSegmentBins
altFracThreshold <- opt$altFracThreshold
ploidy <- eval(parse(text = opt$ploidy))
coverage <- opt$coverage
maxCN <- opt$maxCN
txnE <- opt$txnE
txnStrength <- opt$txnStrength
normalizeMaleX <- as.logical(opt$normalizeMaleX)
includeHOMD <- as.logical(opt$includeHOMD)
minTumFracToCorrect <- opt$minTumFracToCorrect
fracReadsInChrYForMale <- opt$fracReadsInChrYForMale
chrXMedianForMale <- -0.1
outDir <- opt$outDir
libdir <- opt$libdir
plotFileType <- opt$plotFileType
plotYLim <- eval(parse(text=opt$plotYLim))
gender <- NULL
outImage <- paste0(outDir,"/", patientID,".RData")
genomeBuild <- opt$genomeBuild
genomeStyle <- opt$genomeStyle
chrs <- as.character(eval(parse(text = opt$chrs)))
chrTrain <- as.character(eval(parse(text=opt$chrTrain)));
chrNormalize <- as.character(eval(parse(text=opt$chrNormalize)));
seqlevelsStyle(chrs) <- genomeStyle
seqlevelsStyle(chrNormalize) <- genomeStyle
seqlevelsStyle(chrTrain) <- genomeStyle

## load ichorCNA library or source R scripts
if (!is.null(libdir) && libdir != "None"){
	source(paste0(libdir,"/R/utils.R"))
	source(paste0(libdir,"/R/segmentation.R"))
	source(paste0(libdir,"/R/EM.R"))
	source(paste0(libdir,"/R/output.R"))
	source(paste0(libdir,"/R/plotting.R"))
} else {
    library(ichorCNA)
}

## load seqinfo
seqinfo <- getSeqInfo(genomeBuild, genomeStyle)

if (substr(tumour_file,nchar(tumour_file)-2,nchar(tumour_file)) == "wig") {
  wigFiles <- data.frame(cbind(patientID, tumour_file))
} else {
  wigFiles <- read.delim(tumour_file, header=F, as.is=T)
}

## FILTER BY EXONS IF PROVIDED ##
## add gc and map to GRanges object ##
if (is.null(exons.bed) || exons.bed == "None" || exons.bed == "NULL"){
  targetedSequences <- NULL
}else{
  targetedSequences <- read.delim(exons.bed, header=T, sep="\t")
}

## load PoN
if (is.null(normal_panel) || normal_panel == "None" || normal_panel == "NULL"){
	normal_panel <- NULL
}

if (is.null(centromere) || centromere == "None" || centromere == "NULL"){ # no centromere file provided
	centromere <- system.file("extdata", "GRCh37.p13_centromere_UCSC-gapTable.txt",
			package = "ichorCNA")
}
centromere <- read.delim(centromere,header=T,stringsAsFactors=F,sep="\t")
save.image(outImage)
## LOAD IN WIG FILES ##
numSamples <- nrow(wigFiles)

tumour_copy <- list()
for (i in 1:numSamples) {
  id <- wigFiles[i,1]
  ## create output directories for each sample ##
  dir.create(paste0(outDir, "/", id, "/"), recursive = TRUE)
  ### LOAD TUMOUR AND NORMAL FILES ###
  message("Loading tumour file:", wigFiles[i,1])
  tumour_reads <- wigToGRanges(wigFiles[i,2])

  ## LOAD GC/MAP WIG FILES ###
  # find the bin size and load corresponding wig files #
  binSize <- as.data.frame(tumour_reads[1,])$width
  message("Reading GC and mappability files")
  if (is.null(gcWig) || gcWig == "None" || gcWig == "NULL"){
      stop("GC wig file is required")
  }
  gc <- wigToGRanges(gcWig)
  if (is.null(mapWig) || mapWig == "None" || mapWig == "NULL"){
      message("No mappability wig file input, excluding from correction")
      map <- NULL
  } else {
      map <- wigToGRanges(mapWig)
  }
  message("Correcting Tumour")

  counts <- loadReadCountsFromWig(tumour_reads, chrs = chrs, gc = gc, map = map,
                                       centromere = centromere, flankLength = flankLength,
                                       targetedSequences = targetedSequences, chrXMedianForMale = chrXMedianForMale,
                                       genomeStyle = genomeStyle, fracReadsInChrYForMale = fracReadsInChrYForMale,
                                       chrNormalize = chrNormalize, mapScoreThres = minMapScore)
  tumour_copy[[id]] <- counts$counts #as(counts$counts, "GRanges")
  gender <- counts$gender
  ## load in normal file if provided
  if (!is.null(normal_file) && normal_file != "None" && normal_file != "NULL"){
	message("Loading normal file:", normal_file)
	normal_reads <- wigToGRanges(normal_file)
	message("Correcting Normal")
	counts <- loadReadCountsFromWig(normal_reads, chrs=chrs, gc=gc, map=map,
			centromere=centromere, flankLength = flankLength, targetedSequences=targetedSequences,
			genomeStyle = genomeStyle, chrNormalize = chrNormalize, mapScoreThres = minMapScore)
	normal_copy <- counts$counts #as(counts$counts, "GRanges")
	gender.normal <- counts$gender
  }else{
	normal_copy <- NULL
  }

  ### DETERMINE GENDER ###
  ## if normal file not given, use chrY, else use chrX
  message("Determining gender...", appendLF = FALSE)
  gender.mismatch <- FALSE
  if (!is.null(normal_copy)){
	if (gender$gender != gender.normal$gender){ #use tumour # use normal if given
	# check if normal is same gender as tumour
	  gender.mismatch <- TRUE
	}
  }
  message("Gender ", gender$gender)

  ## NORMALIZE GENOME-WIDE BY MATCHED NORMAL OR NORMAL PANEL (MEDIAN) ##
  tumour_copy[[id]] <- normalizeByPanelOrMatchedNormal(tumour_copy[[id]], chrs = chrs,
      normal_panel = normal_panel, normal_copy = normal_copy,
      gender = gender$gender, normalizeMaleX = normalizeMaleX)

	### OUTPUT FILE ###
	### PUTTING TOGETHER THE COLUMNS IN THE OUTPUT ###
	outMat <- as.data.frame(tumour_copy[[id]])
	#outMat <- outMat[,c(1,2,3,12)]
	outMat <- outMat[,c("seqnames","start","end","copy")]
	colnames(outMat) <- c("chr","start","end","log2_TNratio_corrected")
	outFile <- paste0(outDir,"/",id,".correctedDepth.txt")
	message(paste("Outputting to:", outFile))
	write.table(outMat, file=outFile, row.names=F, col.names=T, quote=F, sep="\t")

} ## end of for each sample

chrInd <- as.character(seqnames(tumour_copy[[1]])) %in% chrTrain
## get positions that are valid
valid <- tumour_copy[[1]]$valid
if (length(tumour_copy) >= 2) {
  for (i in 2:length(tumour_copy)){
    valid <- valid & tumour_copy[[i]]$valid
  }
}
save.image(outImage)

### RUN HMM ###
## store the results for different normal and ploidy solutions ##
ptmTotalSolutions <- proc.time() # start total timer
results <- list()
loglik <- as.data.frame(matrix(NA, nrow = length(normal) * length(ploidy), ncol = 7,
                 dimnames = list(c(), c("init", "n_est", "phi_est", "BIC",
                 												"Frac_genome_subclonal", "Frac_CNA_subclonal", "loglik"))))
counter <- 1
compNames <- rep(NA, nrow(loglik))
mainName <- rep(NA, length(normal) * length(ploidy))
#### restart for purity and ploidy values ####
for (n in normal){
  for (p in ploidy){
    if (n == 0.95 & p != 2) {
        next
    }
    logR <- as.data.frame(lapply(tumour_copy, function(x) { x$copy })) # NEED TO EXCLUDE CHR X #
    param <- getDefaultParameters(logR[valid & chrInd, , drop=F], maxCN = maxCN, includeHOMD = includeHOMD,
                ct.sc=scStates, ploidy = floor(p), e=txnE, e.same = 50, strength=txnStrength)
    param$phi_0 <- rep(p, numSamples)
    param$n_0 <- rep(n, numSamples)

    ############################################
    ######## CUSTOM PARAMETER SETTINGS #########
    ############################################
    # 0.1x cfDNA #
    if (is.null(lambda)){
			logR.var <- 1 / ((apply(logR, 2, sd, na.rm = TRUE) / sqrt(length(param$ct))) ^ 2)
			param$lambda <- rep(logR.var, length(param$ct))
			param$lambda[param$ct %in% c(2)] <- logR.var
			param$lambda[param$ct %in% c(1,3)] <- logR.var
			param$lambda[param$ct >= 4] <- logR.var / 5
			param$lambda[param$ct == max(param$ct)] <- logR.var / 15
			param$lambda[param$ct.sc.status] <- logR.var / 10
    }else{
			param$lambda[param$ct %in% c(2)] <- lambda[2]
			param$lambda[param$ct %in% c(1)] <- lambda[1]
			param$lambda[param$ct %in% c(3)] <- lambda[3]
			param$lambda[param$ct >= 4] <- lambda[4]
			param$lambda[param$ct == max(param$ct)] <- lambda[2] / 15
			param$lambda[param$ct.sc.status] <- lambda[2] / 10
		}
		param$alphaLambda <- rep(lambdaScaleHyperParam, length(param$ct))
    # 1x bulk tumors #
    #param$lambda[param$ct %in% c(2)] <- 2000
    #param$lambda[param$ct %in% c(1)] <- 1750
    #param$lambda[param$ct %in% c(3)] <- 1750
    #param$lambda[param$ct >= 4] <- 1500
    #param$lambda[param$ct == max(param$ct)] <- 1000 / 25
		#param$lambda[param$ct.sc.status] <- 1000 / 75
		#param$alphaLambda[param$ct.sc.status] <- 4
		#param$alphaLambda[param$ct %in% c(1,3)] <- 5
		#param$alphaLambda[param$ct %in% c(2)] <- 5
		#param$alphaLambda[param$ct == max(param$ct)] <- 4

		#############################################
		################ RUN HMM ####################
		#############################################
    hmmResults.cor <- HMMsegment(tumour_copy, valid, dataType = "copy",
                                 param = param, chrTrain = chrTrain, maxiter = 50,
                                 estimateNormal = estimateNormal, estimatePloidy = estimatePloidy,
                                 estimateSubclone = estimateScPrevalence, verbose = TRUE)

    for (s in 1:numSamples){
  		iter <- hmmResults.cor$results$iter
  		id <- names(hmmResults.cor$cna)[s]

  		## convert full diploid solution (of chrs to train) to have 1.0 normal or 0.0 purity
  		## check if there is an altered segment that has at least a minimum # of bins
  		segsS <- hmmResults.cor$results$segs[[s]]
  		segsS <- segsS[segsS$chr %in% chrTrain, ]
  		segAltInd <- which(segsS$event != "NEUT")
  		maxBinLength = -Inf
  		if (sum(segAltInd) > 0){
  			maxInd <- which.max(segsS$end[segAltInd] - segsS$start[segAltInd] + 1)
  			maxSegRD <- GRanges(seqnames=segsS$chr[segAltInd[maxInd]],
  								ranges=IRanges(start=segsS$start[segAltInd[maxInd]], end=segsS$end[segAltInd[maxInd]]))
  			hits <- findOverlaps(query=maxSegRD, subject=tumour_copy[[s]][valid, ])
  			maxBinLength <- length(subjectHits(hits))
  		}
  		## check if there are proportion of total bins altered
  		# if segment size smaller than minSegmentBins, but altFrac > altFracThreshold, then still estimate TF
  		cnaS <- hmmResults.cor$cna[[s]]
  		altInd <- cnaS[cnaS$chr %in% chrTrain, "event"] == "NEUT"
  		altFrac <- sum(!altInd, na.rm=TRUE) / length(altInd)
  		if ((maxBinLength <= minSegmentBins) & (altFrac <= altFracThreshold)){
  			hmmResults.cor$results$n[s, iter] <- 1.0
  		}

      # correct integer copy number based on estimated purity and ploidy
      correctedResults <- correctIntegerCN(cn = hmmResults.cor$cna[[s]],
            segs = hmmResults.cor$results$segs[[s]],
            purity = 1 - hmmResults.cor$results$n[s, iter], ploidy = hmmResults.cor$results$phi[s, iter],
            cellPrev = 1 - hmmResults.cor$results$sp[s, iter],
            maxCNtoCorrect.autosomes = maxCN, maxCNtoCorrect.X = maxCN, minPurityToCorrect = minTumFracToCorrect,
            gender = gender$gender, chrs = chrs, correctHOMD = includeHOMD)
      hmmResults.cor$results$segs[[s]] <- correctedResults$segs
      hmmResults.cor$cna[[s]] <- correctedResults$cn

      	## plot solution ##
  		outPlotFile <- paste0(outDir, "/", id, "/", id, "_genomeWide_", "n", n, "-p", p)
  		mainName[counter] <- paste0(id, ", n: ", n, ", p: ", p, ", log likelihood: ", signif(hmmResults.cor$results$loglik[hmmResults.cor$results$iter], digits = 4))
  		plotGWSolution(hmmResults.cor, s=s, outPlotFile=outPlotFile, plotFileType=plotFileType,
            logR.column = "logR", call.column = "Corrected_Call",
  					 plotYLim=plotYLim, estimateScPrevalence=estimateScPrevalence, seqinfo=seqinfo, main=mainName[counter])
    }
    iter <- hmmResults.cor$results$iter
    results[[counter]] <- hmmResults.cor
    loglik[counter, "loglik"] <- signif(hmmResults.cor$results$loglik[iter], digits = 4)
    subClonalBinCount <- unlist(lapply(hmmResults.cor$cna, function(x){ sum(x$subclone.status) }))
    fracGenomeSub <- subClonalBinCount / unlist(lapply(hmmResults.cor$cna, function(x){ nrow(x) }))
    fracAltSub <- subClonalBinCount / unlist(lapply(hmmResults.cor$cna, function(x){ sum(x$copy.number != 2) }))
    fracAltSub <- lapply(fracAltSub, function(x){if (is.na(x)){0}else{x}})
    loglik[counter, "Frac_genome_subclonal"] <- paste0(signif(fracGenomeSub, digits=2), collapse=",")
    loglik[counter, "Frac_CNA_subclonal"] <- paste0(signif(as.numeric(fracAltSub), digits=2), collapse=",")
    loglik[counter, "init"] <- paste0("n", n, "-p", p)
    loglik[counter, "n_est"] <- paste(signif(hmmResults.cor$results$n[, iter], digits = 2), collapse = ",")
    loglik[counter, "phi_est"] <- paste(signif(hmmResults.cor$results$phi[, iter], digits = 4), collapse = ",")

    counter <- counter + 1
  }
}
## get total time for all solutions ##
elapsedTimeSolutions <- proc.time() - ptmTotalSolutions
message("Total ULP-WGS HMM Runtime: ", format(elapsedTimeSolutions[3] / 60, digits = 2), " min.")

### SAVE R IMAGE ###
save.image(outImage)
#save(tumour_copy, results, loglik, file=paste0(outDir,"/",id,".RData"))

### SELECT SOLUTION WITH LARGEST LIKELIHOOD ###
loglik <- loglik[!is.na(loglik$init), ]
if (estimateScPrevalence){ ## sort but excluding solutions with too large % subclonal
	fracInd <- which(loglik[, "Frac_CNA_subclonal"] <= maxFracCNASubclone &
						 		   loglik[, "Frac_genome_subclonal"] <= maxFracGenomeSubclone)
	if (length(fracInd) > 0){ ## if there is a solution satisfying % subclonal
		ind <- fracInd[order(loglik[fracInd, "loglik"], decreasing=TRUE)]
	}else{ # otherwise just take largest likelihood
		ind <- order(as.numeric(loglik[, "loglik"]), decreasing=TRUE)
	}
}else{#sort by likelihood only
  ind <- order(as.numeric(loglik[, "loglik"]), decreasing=TRUE)
}

#new loop by order of solutions (ind)
outPlotFile <- paste0(outDir, "/", id, "/", id, "_genomeWide_all_sols")
for(i in 1:length(ind)) {
  hmmResults.cor <- results[[ind[i]]]
  turnDevOff <- FALSE
  turnDevOn <- FALSE
  if (i == 1){
  	turnDevOn <- TRUE
  }
  if (i == length(ind)){
  	turnDevOff <- TRUE
  }
  plotGWSolution(hmmResults.cor, s=s, outPlotFile=outPlotFile, plotFileType="pd",
                     logR.column = "logR", call.column = "Corrected_Call",
                     plotYLim=plotYLim, estimateScPrevalence=estimateScPrevalence,
                     seqinfo = seqinfo,
                     turnDevOn = turnDevOn, turnDevOff = turnDevOff, main=mainName[ind[i]])
}

hmmResults.cor <- results[[ind[1]]]
hmmResults.cor$results$loglik <- as.data.frame(loglik)
hmmResults.cor$results$gender <- gender$gender
hmmResults.cor$results$chrYCov <- gender$chrYCovRatio
hmmResults.cor$results$chrXMedian <- gender$chrXMedian
hmmResults.cor$results$coverage <- coverage

outputHMM(cna = hmmResults.cor$cna, segs = hmmResults.cor$results$segs,
                      results = hmmResults.cor$results, patientID = patientID, outDir=outDir)
outFile <- paste0(outDir, "/", patientID, ".params.txt")
outputParametersToFile(hmmResults.cor, file = outFile)

#+end_src
**** Hold and test
:PROPERTIES:
:header-args:snakemake: :tangle no
:ID:       015e0fa3-e595-4898-b7e9-1044fad17195
:END:
***** Ideas
:PROPERTIES:
:header-args:snakemake: :tangle no
:ID:       52812cc2-fc33-46cb-95e9-80aee5c38d7a
:END:

***** Filter fragments by length                                   :smk_rule:
:PROPERTIES:
:ID:       802ed819-7b4e-4caf-9491-6d6e6c3fa469
:END:
- [[./workflow/cna.smk::rule cfdna_cna_frag_filt][Snakemake]]
  #+begin_src snakemake
# Filter fragments by length
rule cna_frag_filt_tmp:
    benchmark: benchdir + "/{library}_{frag_distro}_frag_frag_filt.benchmark.txt",
    container: frag_container,
    input: frag_cna_in_bams + "/{library}.bam",
    log: logdir + "/{library}_{frag_distro}_frag_frag_filt.log",
    output:
        nohead = temp(frag_cna_frag_bams) + "/{library}_frag{frag_distro}.nohead",
        onlyhead = temp(frag_cna_frag_bams) + "/{library}_frag{frag_distro}.only",
        final = frag_cna_frag_bams + "/{library}_frag{frag_distro}.bam",
    params:
        script = "{frag_script_dir}/frag_filt.sh",
        threads = frag_threads,
    shell:
        """
        frag_min=$(echo {wildcards.frag_distro} | sed -e "s/_.*$//g")
        frag_max=$(echo {wildcards.frag_distro} | sed -e "s/^.*_//g")
        {params.script} \
        {input} \
        {output.nohead} \
        $frag_min \
        $frag_max \
        {config[threads]} \
        {output.onlyhead} \
        {output.final}
        """
#+end_src

**** Reference                                                          :ref:
:PROPERTIES:
:ID:       d6909085-d7cc-4a43-95e2-fa364680b576
:END:
***** [[46270062-e3f4-46c9-9d71-5868376e495b][smk yas]]
:PROPERTIES:
:ID:       7f732c2d-27b5-4c95-acea-2f44fe268925
:END:
***** [[file:./workflow/cna.smk][Link to Snakefile]]
:PROPERTIES:
:ID:       3739f63c-1a6c-4359-9730-dff71e88b33e
:END:
** Ideas
:PROPERTIES:
:ID:       8052e364-df54-4c6f-a34f-fe2df8bf07b4
:END:
- [ ] update repos w/ alex pr https://mail.google.com/mail/u/0/#inbox/FMfcgzGrbbvPZCJdHScVHjnFTTtztFDv
*** Multiple frag distros
:PROPERTIES:
:ID:       aff2bea4-dd71-4205-a596-a59c2b2e1a0f
:END:
note- this would require re-write of ds_cond_target_list
*** Testing inputs update
:PROPERTIES:
:ID:       60c34d59-8eab-4d7b-82ee-7f15bf79e729
:END:
- Make a smaller fasta for indexing
  #+begin_src bash
#!/bin/echo For documentation, not intended to be executable:.
singularity shell ~/sing_containers/biotools.1.0.2.sif
repo=/home/jeszyman/repos/cfdna-wgs
wget --directory-prefix="${repo}/test/inputs/" https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

zcat "test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz" | grep -A 1000000 chr8 | gzip > test/inputs/chr8.fa.gz

# Test indexed size
mkdir -p /tmp/testbwa
bwa index -p /tmp/testbwa/chr8 test/inputs/chr8.fa.gz

rm ${repo}/test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

#+end_src
- Make chr8-specific ichor references
  #+begin_src bash
singularity shell ~/sing_containers/frag.1.0.0.sif

~/wigToBigWig -clip /opt/ichorCNA/inst/extdata/gc_hg38_1000kb.wig test/inputs/hg38.chrom.sizes test/inputs/gc_chr8_1000kb.bw

bigWigToWig -chrom=chr8 test/inputs/gc_chr8_1000kb.bw test/inputs/gc_chr8_1000kb.wig

~/wigToBigWig -clip /opt/ichorCNA/inst/extdata/map_hg38_1000kb.wig test/inputs/hg38.chrom.sizes test/inputs/map_chr8_1000kb.bw

bigWigToWig -chrom=chr8 test/inputs/map_chr8_1000kb.bw test/inputs/map_chr8_1000kb.wig

~/wigToBigWig -clip /opt/ichorCNA/inst/extdata/gc_hg38_1000kb.wig test/inputs/hg38.chrom.sizes test/inputs/gc_chr8_1000kb.bw

bigWigToWig -chrom=chr8 test/inputs/gc_chr8_1000kb.bw test/inputs/gc_chr8_1000kb.wig

#+end_src
- ?
  #+begin_src bash
wget --directory-prefix="/home/jeszyman/repos/cfdna-wgs/test/inputs" https://hgdownload.cse.ucsc.edu/goldenpath/hg38/bigZips/hg38.chrom.sizes

wget --directory-prefix="/home/jeszyman/repos/cfdna-wgs/test/inputs" https://raw.githubusercontent.com/Boyle-Lab/Blacklist/master/lists/hg38-blacklist.v2.bed.gz

gunzip -c ~/repos/cfdna-wgs/test/inputs/hg38-blacklist.v2.bed.gz > ~/repos/cfdna-wgs/test/inputs/hg38-blacklist.v2.bed

#+end_src
  #+begin_src bash
singularity shell --bind /mnt ~/sing_containers/frag.1.0.0.sif

# Clear bam directory if present
if [ -r test/bam ]; then \rm -rf test/bam; fi
mkdir -p test/bam

# Create small bam files to store in repo. Subsample real bams to ~100 Mb.
sambamba view -s .005 -f bam -t 36 /mnt/ris/aadel/Active/mpnst/test/bam/new_HiSeq15_L002001_ACAC_extract_ds20.bam > test/inputs/lib003_hg38.bam
sambamba view -s .005 -f bam -t 36 /mnt/ris/aadel/Active/mpnst/test/bam/new_HiSeq15_L002001_ATCG_extract_ds20.bam > test/inputs/lib004_hg38.bam
sambamba view -s 0.01 -f bam -t 4 /mnt/ris/aadel/Active/mpnst/bam/frag/ds/lib105_ds10.bam > test/inputs/lib005.bam
sambamba view -s 0.01 -f bam -t 4 /mnt/ris/aadel/Active/mpnst/bam/frag/ds/lib205_ds10.bam > test/inputs/lib006.bam

for file in test/inputs/*.bam; do samtools index $file; done

#+end_src
  #+begin_src bash
mkdir -p ~/repos/cfdna-wgs/test/analysis/cfdna_frag_bams
cp ~/repos/cfdna-frag/test/bam/frag/*.bam ~/repos/cfdna-wgs/test/analysis/cfdna_frag_bams/
#+end_src
  #+begin_src bash
singularity shell --bind /mnt ~/sing_containers/frag.1.0.0.sif

# Get hg38 gc bigwig
wget --directory-prefix /tmp/ http://hgdownload.cse.ucsc.edu/gbdb/hg38/bbi/gc5BaseBw/gc5Base.bw

# convert hg38 gc bigwig to tsv binned at 5 Mb (like Mathios, 2021)
multiBigwigSummary bins \
    --binSize 5000000 \
    --bwfiles /tmp/gc5Base.bw \
    --numberOfProcessors 4 \
    --outFileName /tmp/test.out \
    --outRawCounts /tmp/gc5mb.tsv

tail -n +2 /tmp/gc5mb.tsv > test/inputs/gc5mb.bed




#+end_src
  - bedtools subtract -a "test/inputs/chr8.bed" -b "test/inputs/hg38-blacklist.v2.bed" > "test/inputs/keep.bed"

*** Make example fastq more efficient-
:PROPERTIES:
:ID:       9c71dd59-31c8-4a88-a7ac-ee8847bf048c
:END:
- Find reads that map to early 8 and work back into a fastq
*** filter CNA abberant regions
:PROPERTIES:
:ID:       71679fee-4d06-492f-9f2e-93a93514b380
:END:
- cite:dehner2021
*** Check / explain - Prerequisites for local integration testing
:PROPERTIES:
:ID:       752985f3-8b2b-4f60-9df1-401278edb16a
:END:
*** update /simplify aggregate qc table
:PROPERTIES:
:ID:       310ae928-1a05-4531-b6e8-e0607bb16cb2
:END:
*** expand seq depth metrics
:PROPERTIES:
:ID:       99ef5f61-98b9-4874-95f1-406bdce98b3c
:END:
  - using mosdepth
    #+name: mosdepth
    #+begin_src bash
  #########1#########2#########3#########4#########5#########6#########7#########8
  #
  ### mosdepth for WGS depth calc  ###
  #
  # Setup
  ##

  # Mosdepth per bam dir
  ##
  ## For deduped bams
  for file in $localdata/bams/*.dedup.sorted.bam; do
      mosdepth_mpnst $file $localdata/bam-qc/dedup 250000000
  done
  ##
  #
  # get simple tsv and send to repo

  for file in $localdata/bam-qc/dedup/lib*.regions.bed.gz; do
      base=`basename -s .dedup.sorted.regions.bed.gz $file`
      zcat $file | awk -v FS='\t' -v var=$base 'NR <=24 {print var,$1,$4}' >> $localdata/bam-qc/dedup/all_dedup_coverage
  done

  header=library_id\\tchr\\tmean_coverage
  sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

  ## Local
  >>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228
  source ~/repos/mpnst/bin/local-setup.sh
  docker_interactive
  biotools
  ##
  ## Functions
  ###
  ### Convert bams to wigs
  bam_to_wig() {
      printf "Variables are: 1=bam_file 2=bam_suffix 3=outdir\n"
          base=`basename -s ${2} $1`
          if [ $3/${base}.wig -ot $1 ]; then
              /opt/hmmcopy_utils/bin/readCounter --window 1000000 --quality 20 \
                                                 --chromosome "chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chrX,chrY" $1 > $3/${base}.wig
          fi
  }
  ###
  ### Run ichor for low TF
  ichor_lowfract() {
      base=`basename -s .wig $1`
      if [ $2/$base.RData -ot $1 ]; then
          Rscript /opt/ichorCNA/scripts/runIchorCNA.R \
                  --id $base \
                  --WIG $1 \
                  --gcWig /opt/ichorCNA/inst/extdata/gc_hg19_1000kb.wig \
                  --normal "c(0.95, 0.99, 0.995, 0.999)" \
                  --ploidy "c(2)" \
                  --maxCN 3 \
                  --estimateScPrevalence FALSE \
                  --scStates "c()" \
                  --outDir $2
      fi
  }
  ##
  ##
  mkdir -p $localdata/wigs
  mkdir -p $localdata/ichor
  #
  # Make wigs
  #
  #bam_to_wig /mnt/xt3/mpnst/frag-filt-bams/lib109.dedup.sorted.frag90_150.sorted.bam .dedup.sorted.frag90_150.sorted.bam $localdata/wigs
  ##
  for file in $localdata/frag-filt-bams/lib109*.bam; do
      bam_to_wig $file \
                 .dedup.sorted.frag.sorted.bam \
                 $localdata/wigs
  done

  ## For fraction-filtered WGS cfDNA
  for file in $localdata/frag-filt-bams/*.bam; do
      bam_to_wig $file \
                 .dedup.sorted.frag.sorted.bam \
                 $localdata/wigs
  done
  ##
  ## For tumor and leukocyte WGS libraries
  ### Make array of genomic library file paths
  genomic=($(cat /drive3/users/jszymanski/repos/mpnst/data/libraries.csv | grep -e tumor -e leukocyte | grep -v "wes" | awk -F, '{print $1}' | sed 's/"//g' | sed 's/$/.dedup.sorted.bam/g' | sed 's/^/\/mnt\/xt3\/mpnst\/bams\//g'))
  ###
  for file in ${genomic[@]}; do
      bam_to_wig $file \
                 .dedup.sorted.bam \
                 $localdata/wigs
  done
  #
  ##
  ## Send successful file list to repo
  rm /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
  for file in $localdata/wigs/*.wig;
  do
      base=`basename -s .wig $file`
      echo $base >> /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
  done
  #
  ##RESUME HERE
  # ichor
  ##
  for file in $localdata/wigs/lib109*.wig; do
      ichor_lowfract $file $localdata/ichor
  done


  header=library_id\\tchr\\tmean_coverage
  sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

  max_file_size=5000000
  file_size=$(
      wc -c <"$localdata/bam-qc/dedup/all_dedup_coverage"
           )

  if [ $filesize -gt $max_file_size ]; then
      touch $repo/data/qc/all_dedup_coverage_too_big
  else
      cp $localdata/bam-qc/dedup/all_dedup_coverage $repo/qc/all_dedup_coverage.tsv
  fi
  #
  #+end_src
    - Cant calcualte depths off [[file:~/repos/mpnst/data/bam_qc_data/mqc_mosdepth-coverage-per-contig_1.txt]] , d/n allow values under 1
    - [ ] for coverage, should intersect down to autosomes
    - https://github.com/brentp/mosdepth
    - run and extract mosdepth
      mosdepthRAW = as_tibble(read.table(file.path(repo,"data/all_dedup_coverage.tsv"), header = T, sep = '\t', fill = TRUE))
*** make consolidated per-cna file (see $data_dir/old/all_cna.bed)
:PROPERTIES:
:ID:       35acec46-a23e-46b9-a5df-f0ab5a9c4385
:END:

*** Bam  downsampling multiple downsampling sizes
:PROPERTIES:
:ID:       4efbd823-db02-41ab-bab5-97f75abf8c25
:END:

**** Downsample bams                                               :smk_rule:
:PROPERTIES:
:ID:       f447b30c-96dc-4969-b071-5baab4a5bca8
:END:
- [[./workflow/reads.smk::rule frag_downsample][Snakemake]]
  #+begin_src snakemake
# Downsample bam file to a set number of reads
rule frag_downsample:
    benchmark: benchdir + "/{library}_{milreads}_frag_downsample.benchmark.txt",
    container: frag_container,
    input: frag_bams + "/{library}_filt.bam",
    log: logdir + "/{library}_{milreads}_frag_downsample.log",
    output: frag_bams + "/{library}_ds{milreads}.bam",
    params:
        milreads = MILREADS,
        script = "{frag_script_dir}/downsample.sh",
        threads = frag_threads,
    shell:
        """
        {params.script} \
        {input} \
        {wildcards.milreads} \
        {output} &> {log}
        """
#+end_src
- [[file:./scripts/downsample.sh][Shell script]]
  #+begin_src bash
#!/usr/bin/env bash

# For unit testing
# in_bam="test/analysis/frag_bams/lib001_filt.bam"
# out_bam=/tmp/test.bam
# milreads="0.0041"

in_bam=$1
milreads="$2"
out_bam=$3

reads=$(echo |awk -v var1=$milreads '{ print 1000000*var1 }')

## Calculate the sampling factor based on the intended number of reads:

FACTOR=$(samtools idxstats $in_bam | cut -f3 | awk -v COUNT=$reads 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then
    echo "DS reads exceeds total for $in_bam"
else
    sambamba view -s $FACTOR -f bam -l 5 $in_bam > $out_bam
fi

#+end_src
** Reference                                                            :ref:
:PROPERTIES:
:header-args: :tangle no
:ID:       80bff0e2-0a40-46c8-bc63-15eff53ad00c
:END:
- https://github.com/jeszyman/cfdna-wgs
*** [[id:271b4d5f-727e-496e-b835-8fe9f8655655][Bioinformatics project module]]
:PROPERTIES:
:ID:       66399318-4be5-4404-8f19-c872abb15255
:END:
*** [[https://github.com/jeszyman/cfdna-wgs][Github link]]
:PROPERTIES:
:ID:       801992bf-8dcd-4df6-a0c2-9778829e924d
:END:
*** [[id:13120759-71db-497c-8ed3-1c58e47a7840][Biotools headline]]
:PROPERTIES:
:ID:       43d7175a-a16e-4bde-8f5a-b59985de3b42
:END:
*** Old rules
:PROPERTIES:
:ID:       a6406c59-ea89-4d45-ac8e-6e787d145865
:END:
**** DONE Alignment processing                                     :smk_rule:
:PROPERTIES:
:ID:       e00df395-b730-493c-959c-79e5ca492420
:END:
#+begin_src snakemake
# Alignment deduplication and sorting
rule alignment_processing:
    input:
        config["data_dir"] + "/bam/{library_id}_raw.bam",
    output:
        dedup = temp(config["data_dir"] + "/bam/{library_id}_dedup_unsort.bam"),
        sort = config["data_dir"] + "/bam/{library_id}_dedup.bam",
        index = config["data_dir"] + "/bam/{library_id}_dedup.bam.bai",
    log:
        config["data_dir"] + "/logs/alignment_processing_{library_id}.log"
    shell:
        """
        {config[frag_script_dir]}/alignment_processing.sh \
        {input} \
        {config[threads]} \
        {output.bam} \
        {output.dedup} \
        {output.sort} \
        {output.index} \
        &> {log}
        """
#+end_src
- [[file:workflow/scripts/alignment_processing.sh][Script]]
  #+begin_src bash
#!/usr/bin/env bash

<#bash_preamble#>

input=$1
threads=$2
output_bam=$3
output_dedup=$4
output_sort=$5
output_index=$6

sambamba view -t $threads -S -f bam $input > $output_bam
sambamba markdup -r -t $threads $output_bam $output_dedup
sambamba sort -t $threads $output_dedup -o $output_sort
sambamba index -t $threads $output_sort

#+end_src
*** Dockerfile
:PROPERTIES:
:header-args:bash: :tangle ./docker/frag_Dockerfile
:ID:       b10eb670-a855-4591-a74c-b458cb5f0581
:END:
**** Preamble
:PROPERTIES:
:ID:       dec91c64-7d9d-48be-80fd-00973e9b3d4d
:END:
#+begin_src bash
FROM jeszyman/biotools:1.0.2

#################
###   Notes   ###
#################
#
# After build, the image will be pushed to the dockerhub as
# jeszyman/frag
# (https://hub.docker.com/repository/docker/jeszyman/frag)

#+end_src
**** IchorCNA
:PROPERTIES:
:ID:       4f7fe7be-f7e9-45c4-9910-455bd5c4793c
:END:
#+begin_src bash

#
RUN cd /opt && \
    git clone https://github.com/shahcompbio/hmmcopy_utils.git && \
    cd hmmcopy_utils && \
    cmake . && \
    make

#
# ichorCNA
##
## linux dependencies
RUN apt-get update \
   && apt-get install -y \
   libcurl4-openssl-dev \
   libssl-dev \
   libxml2-dev
#RUN rm /usr/lib/x86_64-linux-gnu/libcurl.so.4
#RUN ln -s /usr/lib/x86_64-linux-gnu/libcurl.so.4.5.0 /usr/lib/x86_64-linux-gnu/libcurl.so.4
##
## R dependencies
RUN R -e 'install.packages("BiocManager"); BiocManager::install(); BiocManager::install("HMMcopy"); BiocManager::install("GenomeInfoDb"); BiocManager::install("GenomicRanges");'
##
## git clone install
RUN cd /opt \
    && git clone https://github.com/broadinstitute/ichorCNA.git \
    && cd ichorCNA \
    && R CMD INSTALL . \
    && cd /opt
##

#+end_src
